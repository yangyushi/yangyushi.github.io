<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>NLP with Tensorflow</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="stylesheet" href="/assets/css/styles/github-gist.css">
    <script src="/assets/js/toc.js"></script>
    <script src="/assets/js/highlight.pack.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>

    <script type="text/javascript" id="MathJax-script" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
      >
    </script>
    
    </head>

  <body>
    <script>hljs.initHighlightingOnLoad();</script>
    
    <nav>
    <ul>
        
        <li>
        <a href=/
            
        >
            About
        </a>
        </li>
        
        <li>
        <a href=/notebook.html
            
        >
            Notebook
        </a>
        </li>
        
    </ul>
</nav>


    <div class="main">
        <div id="side_bar">
    02 Apr 2022
</div>

<div class="post center">
    <ul id="markdown-toc">
  <li><a href="#introducing-the-task" id="markdown-toc-introducing-the-task">Introducing the Task</a></li>
  <li><a href="#convert-texts-to-numbers-tokenise" id="markdown-toc-convert-texts-to-numbers-tokenise">Convert Texts to Numbers (Tokenise)</a></li>
  <li><a href="#learning-the-embedding" id="markdown-toc-learning-the-embedding">Learning the Embedding</a></li>
  <li><a href="#using-the-sequence-model" id="markdown-toc-using-the-sequence-model">Using the Sequence Model</a></li>
  <li><a href="#use-lstm" id="markdown-toc-use-lstm">Use LSTM</a></li>
  <li><a href="#use-1d-convonlution-layer" id="markdown-toc-use-1d-convonlution-layer">Use 1D Convonlution Layer</a></li>
  <li><a href="#generate-new-text" id="markdown-toc-generate-new-text">Generate New Text</a></li>
</ul>

<hr />

<p>This is the note I took when taking <a href="https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome">this coursera course</a>. It is a serious of introductory and helpful lectures.</p>

<hr />

<h2 id="introducing-the-task">Introducing the Task</h2>

<p>To illustrate the concepts for natural language processing (NLP), we will have to use an example throughout the note.</p>

<p>Now we want to learn to predict if a movie comment is a positive comment or negative. Our dataset would contains,</p>

<ul>
  <li>$n$ Comments, and</li>
  <li>Each comment will contain $m$ words, and</li>
  <li>Each comment will be either <strong>positive</strong> or <strong>negative</strong>.</li>
</ul>

<h2 id="convert-texts-to-numbers-tokenise">Convert Texts to Numbers (Tokenise)</h2>

<p>To analyse the natural languages, we need to convert text to numbers. We treat every comment in the same way. And for each comment, we will</p>

<ol>
  <li>Give each word a number/code. This is called <strong>encoding</strong>.</li>
  <li>We decide what to do if we see a new word, known as outer vocabulary (oov).</li>
  <li>We will <strong>pad</strong> the comment so that all of them have equal length, ie word counts.</li>
</ol>

<p>This is relevant code in tensorflow using the class <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer">tokenisor</a> and function <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences">pad_sequences</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequence</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># only give the top 10,000 most frequent words a number/code
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">120</span>    <span class="c1"># maximum length (word count) for each comment
</span><span class="n">trunc_type</span><span class="o">=</span><span class="sh">'</span><span class="s">post</span><span class="sh">'</span>   <span class="c1"># removing excess words at the end of the comments
</span><span class="n">oov_tok</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;OOV&gt;</span><span class="sh">"</span>   <span class="c1"># use &lt;OOV&gt; to represnt words never seen before
</span>
<span class="n">comments</span> <span class="o">=</span> <span class="p">...</span>  <span class="c1"># a list of n comments (strings), each comment has different length
</span>
<span class="c1"># generate the encoding rule that convert words to numbers
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="n">oov_tok</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>

<span class="c1"># convert words to numbers
</span><span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>

<span class="c1"># making the sequences so that they have equal length
</span><span class="n">padded</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="n">trunc_type</span><span class="p">)</span>

<span class="c1"># obtain the map between words and numbers, the dict is {words: number}
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>
</code></pre></div></div>

<h2 id="learning-the-embedding">Learning the Embedding</h2>

<p>The tokenised sequences does not make much scence. Word <em>cat</em> may have the code of <code class="language-plaintext highlighter-rouge">00001</code> and word <em>cats</em> might have <code class="language-plaintext highlighter-rouge">95843</code> but these two words should somehow be similar.</p>

<p>To get meaningful representations of words, we need to represnt the words as vectores. And the words with similar meanings should be <strong>closer</strong> in their corresponding space, where the vectors be in. The conversion from <strong>code</strong> to <strong>vector</strong> is carried out by <strong>embedding</strong>.</p>

<p>We get the embedding by <em>training a predictive network</em>. Here is the relevent code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># only give the top 10,000 most frequent words a number/code
</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># each word is represented by 16 numbers
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">120</span>    <span class="c1"># maximum length (word count) for each comment
</span><span class="n">trunc_type</span><span class="o">=</span><span class="sh">'</span><span class="s">post</span><span class="sh">'</span>   <span class="c1"># removing excess words at the end of the comments
</span><span class="n">oov_tok</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;OOV&gt;</span><span class="sh">"</span>   <span class="c1"># use &lt;OOV&gt; to represnt words never seen before
</span>
<span class="n">comments</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">comment</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">hello world</span><span class="sh">"</span><span class="p">]</span>  <span class="c1"># n comments (strings) with different lengths
</span>
<span class="c1"># generate the encoding rule that convert words to numbers
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="n">oov_tok</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>

<span class="c1"># convert words to numbers
</span><span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>

<span class="c1"># making the sequences so that they have equal length
</span><span class="n">padded</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="n">trunc_type</span><span class="p">)</span>

<span class="c1"># obtain the map between words and numbers, the dict is {words: number}
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>

<span class="c1"># construct a model with embedding
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">binary_crossentropy</span><span class="sh">'</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>

</code></pre></div></div>

<p>This is the summary of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Model</span><span class="p">:</span> <span class="sh">"</span><span class="s">sequential</span><span class="sh">"</span>
<span class="n">_________________________________________________________________</span>
<span class="nc">Layer </span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>                 <span class="n">Output</span> <span class="n">Shape</span>              <span class="n">Param</span> <span class="c1">#   
</span><span class="o">=================================================================</span>
<span class="nf">embedding </span><span class="p">(</span><span class="n">Embedding</span><span class="p">)</span>        <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>           <span class="mi">160000</span>     
<span class="c1"># meaning: 120 vectors, each vector is 16 dimensional
</span><span class="n">_________________________________________________________________</span>
<span class="nf">flatten </span><span class="p">(</span><span class="n">Flatten</span><span class="p">)</span>            <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1920</span><span class="p">)</span>              <span class="mi">0</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">dense </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>                <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>                 <span class="mi">11526</span> 
<span class="n">_________________________________________________________________</span>
<span class="nf">dense_1 </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                 <span class="mi">7</span>
<span class="o">=================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">171</span><span class="p">,</span><span class="mi">533</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">171</span><span class="p">,</span><span class="mi">533</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">_________________________________________________________________</span>
</code></pre></div></div>

<p>The final “product” of the embeding layer is the <strong>weight</strong>. In this exampe, the shape of the weight can be obtained via,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># only give the top 10,000 most frequent words a number/code
</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># each word is represented by 16 numbers
</span><span class="p">....</span><span class="bp">...</span>
<span class="n">embed_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">embed_layer</span><span class="p">.</span><span class="nf">weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># shape (10000, 16), 
</span></code></pre></div></div>

<p>The weights will map every word from their <strong>code</strong> to a <strong>vector</strong>.</p>

<h2 id="using-the-sequence-model">Using the Sequence Model</h2>

<p>We need consider the <strong>sequence</strong> of the language into consideration. We use recurrent neural network (RNN) to consider the sequence. RNN can be graphically represented like the following figure.</p>

<pre class="asciiart">
       .───────────.       
      (    y(t)     )      
       `───────────'       
             ▲             
┌────────────┼────────────┐
│            │            │
│  ┌───────────────────┐  │
└─▶│     Function      │──┘
   └───────────────────┘   
             ▲             
             │             
             │             
       .───────────.       
      (    x(t)     )      
       `───────────'       
</pre>

<p>The RNN can be “expanded” into a more understandable version.</p>

<pre class="asciiart">
    .───────────.                .───────────.                 .───────────.           
   (    y(0)     )              (    y(1)     )               (    y(2)     )          
    `───────────'                `───────────'                 `───────────'           
          ▲                            ▲                             ▲                 
          │                            │                             │                 
          │                            │                             │                 
┌───────────────────┐        ┌───────────────────┐         ┌───────────────────┐       
│     Function      │───────▶│     Function      │────────▶│     Function      │──────▶
└───────────────────┘        └───────────────────┘         └───────────────────┘       
          ▲                            ▲                             ▲                 
          │                            │                             │                 
          │                            │                             │                 
    .───────────.                .───────────.                 .───────────.           
   (    x(0)     )              (    x(1)     )               (    x(2)     )          
    `───────────'                `───────────'                 `───────────'           
</pre>

<p>Using RNN, the information carried by <code class="language-plaintext highlighter-rouge">x(0)</code> will propagate all the way to later time points.</p>

<p>Unfortunately there is no example code snippest from the course.</p>

<h2 id="use-lstm">Use LSTM</h2>

<p>A very popular model called long short term memory (LSTM) can be used to construct an RNN. The structure of the model is different but the idea is similar.</p>

<pre class="asciiart">
    .───────────.                .───────────.                 .───────────.           
   (    y(0)     )              (    y(1)     )               (    y(2)     )          
    `───────────'                `───────────'                 `───────────'           
          ▲                            ▲                             ▲                 
  ━━━━━━━━╋━━━━━━━Cell State━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━▶    
          │                            │                             │                 
┌───────────────────┐        ┌───────────────────┐         ┌───────────────────┐       
│     Function      │───────▶│     Function      │────────▶│     Function      │──────▶
└───────────────────┘        └───────────────────┘         └───────────────────┘       
          ▲                            ▲                             ▲                 
          │                            │                             │                 
          │                            │                             │                 
    .───────────.                .───────────.                 .───────────.           
   (    x(0)     )              (    x(1)     )               (    x(2)     )          
    `───────────'                `───────────'                 `───────────'           
</pre>

<p>LSTM can be think of as an “update” to RNN. There is an additional pipeline called “cell state” being passed from earilier states. The cell state can also be bi-direcitonal, where the future can affect the past.</p>

<p>The following code use the LSTM in the model in tensorflow.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># only give the top 10,000 most frequent words a number/code
</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># each word is represented by 16 numbers
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">120</span>    <span class="c1"># maximum length (word count) for each comment
</span><span class="n">trunc_type</span><span class="o">=</span><span class="sh">'</span><span class="s">post</span><span class="sh">'</span>   <span class="c1"># removing excess words at the end of the comments
</span><span class="n">oov_tok</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;OOV&gt;</span><span class="sh">"</span>   <span class="c1"># use &lt;OOV&gt; to represnt words never seen before
</span>
<span class="n">comments</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">comment</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">hello world</span><span class="sh">"</span><span class="p">]</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="n">oov_tok</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>
<span class="n">padded</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="n">trunc_type</span><span class="p">)</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>

<span class="c1"># construct a model with sequence model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">)),</span>  <span class="c1"># use LSTM
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">binary_crossentropy</span><span class="sh">'</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>
</code></pre></div></div>

<p>The above code works for <code class="language-plaintext highlighter-rouge">tensorflow 2.6.0</code>, and the output is.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Model</span><span class="p">:</span> <span class="sh">"</span><span class="s">sequential</span><span class="sh">"</span>
<span class="n">_________________________________________________________________</span>
<span class="nc">Layer </span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>                 <span class="n">Output</span> <span class="n">Shape</span>              <span class="n">Param</span> <span class="c1">#   
</span><span class="o">=================================================================</span>
<span class="nf">embedding </span><span class="p">(</span><span class="n">Embedding</span><span class="p">)</span>        <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>           <span class="mi">160000</span>    
<span class="n">_________________________________________________________________</span>
<span class="nf">bidirectional </span><span class="p">(</span><span class="nc">Bidirectional </span><span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>               <span class="mi">41472</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">dense </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>                <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>                <span class="mi">8256</span>      
<span class="n">_________________________________________________________________</span>
<span class="nf">dense_1 </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                 <span class="mi">65</span>        
<span class="o">=================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">209</span><span class="p">,</span><span class="mi">793</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">209</span><span class="p">,</span><span class="mi">793</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">_________________________________________________________________</span>
</code></pre></div></div>

<p>Notice the output shape of hthe <code class="language-plaintext highlighter-rouge">bidirectional</code> layer is <code class="language-plaintext highlighter-rouge">128</code>, and this is because it is <code class="language-plaintext highlighter-rouge">64 * 2</code>, due to the bidirectional nature.</p>

<p>(In the lecture, it is mentioned that a <strong>smooth</strong> accuracy curve is good.)</p>

<h2 id="use-1d-convonlution-layer">Use 1D Convonlution Layer</h2>

<p>Except for the LSTM, we can also use a 1D convolution layer, like below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># construct a model convolution rather than LSTM
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">),</span>
  	<span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv1D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">GlobalMaxPooling1D</span><span class="p">(),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>The summary of the model is</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Model</span><span class="p">:</span> <span class="sh">"</span><span class="s">sequential</span><span class="sh">"</span>
<span class="n">_________________________________________________________________</span>
<span class="nc">Layer </span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>                 <span class="n">Output</span> <span class="n">Shape</span>              <span class="n">Param</span> <span class="c1">#   
</span><span class="o">=================================================================</span>
<span class="nf">embedding </span><span class="p">(</span><span class="n">Embedding</span><span class="p">)</span>        <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>           <span class="mi">160000</span>    
<span class="n">_________________________________________________________________</span>
<span class="nf">conv1d </span><span class="p">(</span><span class="n">Conv1D</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">116</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>          <span class="mi">10368</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">dense </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>                <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">116</span><span class="p">,</span> <span class="mi">64</span><span class="p">)</span>           <span class="mi">8256</span>      
<span class="n">_________________________________________________________________</span>
<span class="nf">dense_1 </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">116</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>            <span class="mi">65</span>        
<span class="o">=================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">178</span><span class="p">,</span><span class="mi">689</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">178</span><span class="p">,</span><span class="mi">689</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">_________________________________________________________________</span>
</code></pre></div></div>

<h2 id="generate-new-text">Generate New Text</h2>

<p>We can train a network to generate new text. The idea is to <em>predict the next word</em> based on existing texts.</p>

<p>The following code present an example for this task.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">()</span>

<span class="n">data</span><span class="o">=</span><span class="sh">"""</span><span class="s">
In the town of Athy one Jeremy Lanigan
Battered away til he hadnt a pound.
His father died and made him a man again
Left him a farm and ten acres of ground.
He gave a grand party for friends and relations
Who didnt forget him when come to the wall,
And if youll but listen Ill make your eyes glisten
Of the rows and the ructions of Lanigans Ball.
Myself to be sure got free invitation,
For all the nice girls and boys I might ask,
And just in a minute both friends and relations
Were dancing round merry as bees round a cask.
Judy ODaly, that nice little milliner,
She tipped me a wink for to give her a call,
And I soon arrived with Peggy McGilligan
Just in time for Lanigans Ball.
There were lashings of punch and wine for the ladies,
Potatoes and cakes; there was bacon and tea,
There were the Nolans, Dolans, OGradys
Courting the girls and dancing away.
Songs they went round as plenty as water,
The harp that once sounded in Taras old hall,
Sweet Nelly Gray and The Rat Catchers Daughter,
All singing together at Lanigans Ball.
They were doing all kinds of nonsensical polkas
All round the room in a whirligig.
Julia and I, we banished their nonsense
And tipped them the twist of a reel and a jig.
Ach mavrone, how the girls got all mad at me
Danced til youd think the ceiling would fall.
For I spent three weeks at Brooks Academy
Learning new steps for Lanigans Ball.
Three long weeks I spent up in Dublin,
Three long weeks to learn nothing at all,
Three long weeks I spent up in Dublin,
Learning new steps for Lanigans Ball.
She stepped out and I stepped in again,
I stepped out and she stepped in again,
She stepped out and I stepped in again,
Learning new steps for Lanigans Ball.
Boys were all merry and the girls they were hearty
And danced all around in couples and groups,
Til an accident happened, young Terrance McCarthy
Put his right leg through miss Finnertys hoops.
Poor creature fainted and cried Meelia murther,
Called for her brothers and gathered them all.
Carmody swore that hed go no further
Til he had satisfaction at Lanigans Ball.
In the midst of the row miss Kerrigan fainted,
Her cheeks at the same time as red as a rose.
Some of the lads declared she was painted,
She took a small drop too much, I suppose.
Her sweetheart, Ned Morgan, so powerful and able,When he saw his fair colleen stretched out by the wall,
Tore the left leg from under the table
And smashed all the Chaneys at Lanigans Ball.
Boys, oh boys, twas then there were runctions.
Myself got a lick from big Phelim McHugh.
I soon replied to his introduction
And kicked up a terrible hullabaloo.
Old Casey, the piper, was near being strangled.
They squeezed up his pipes, bellows, chanters and all.
The girls, in their ribbons, they got all entangled
And that put an end to Lanigans Ball.
</span><span class="sh">"""</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">lower</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">total_words</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># print(tokenizer.word_index) -&gt; 263
# print(total_words) -&gt; {'and': 1, 'the': 2, 'a': 3, ... }
</span>
<span class="n">input_sequences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
	<span class="n">token_list</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">([</span><span class="n">line</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_list</span><span class="p">)):</span>
		<span class="n">n_gram_sequence</span> <span class="o">=</span> <span class="n">token_list</span><span class="p">[:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
		<span class="n">input_sequences</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">n_gram_sequence</span><span class="p">)</span>

<span class="c1"># pad sequences 
</span><span class="n">max_sequence_len</span> <span class="o">=</span> <span class="nf">max</span><span class="p">([</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_sequences</span><span class="p">])</span>
<span class="n">input_sequences</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
  <span class="nf">pad_sequences</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># create predictors and label
</span><span class="n">xs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">input_sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_sequences</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># from numbers to categorical label
</span><span class="n">ys</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">to_categorical</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">total_words</span><span class="p">)</span>

<span class="sh">"""</span><span class="s">
xs[6] -&gt; [ 0  0  0  4  2 66  8 67 68 69] (history)
labels[6] -&gt;  70  (prediction)

categorical label
ys[6].shape -&gt; (263,)
ys[6][70] -&gt; 1
ys[6][65] -&gt; 0
</span><span class="sh">"""</span>

<span class="c1"># Train the network to make prediction
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">total_words</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Bidirectional</span><span class="p">(</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">20</span><span class="p">)))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="n">total_words</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">))</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
  <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Use the network to generate text, starting with she
</span><span class="n">seed_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">She</span><span class="sh">"</span>
<span class="n">next_words</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">next_words</span><span class="p">):</span>
	<span class="n">token_list</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">([</span><span class="n">seed_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
	<span class="n">token_list</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">([</span><span class="n">token_list</span><span class="p">],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">token_list</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	<span class="n">predicted</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
	<span class="n">output_word</span> <span class="o">=</span> <span class="sh">""</span>
	<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
		<span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">predicted</span><span class="p">:</span>
			<span class="n">output_word</span> <span class="o">=</span> <span class="n">word</span>
			<span class="k">break</span>
	<span class="n">seed_text</span> <span class="o">+=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span> <span class="o">+</span> <span class="n">output_word</span>
<span class="nf">print</span><span class="p">(</span><span class="n">seed_text</span><span class="p">)</span>

<span class="c1"># -&gt; She stepped out and i stepped in again again again again
</span></code></pre></div></div>


</div>


    </div>

  </body>

</html>
