<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>数学</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="stylesheet" href="/assets/css/styles/github-gist.css">
    <script src="https://unpkg.com/lunr/lunr.js"></script>

    <script src="/assets/js/toc.js"></script>
    <script src="/assets/js/highlight.pack.js"></script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>

    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

    </head>

  <body>
    <script>hljs.initHighlightingOnLoad();</script>

    <div class="main">
        <div id="side_bar">
    18 Jan 2022
</div>

<div class="center post">
    <ul id="markdown-toc">
  <li><a href="#马尔可夫链" id="markdown-toc-马尔可夫链">马尔可夫链</a>    <ul>
      <li><a href="#定义" id="markdown-toc-定义">定义</a></li>
      <li><a href="#重要的矩阵" id="markdown-toc-重要的矩阵">重要的矩阵</a></li>
      <li><a href="#状态的分类" id="markdown-toc-状态的分类">状态的分类</a></li>
      <li><a href="#定义-1" id="markdown-toc-定义-1">定义</a></li>
      <li><a href="#几何解释" id="markdown-toc-几何解释">几何解释</a></li>
      <li><a href="#与概率的关系" id="markdown-toc-与概率的关系">与概率的关系</a></li>
      <li><a href="#单纯复合体" id="markdown-toc-单纯复合体">单纯复合体</a></li>
    </ul>
  </li>
  <li><a href="#核范数与谱范数" id="markdown-toc-核范数与谱范数">核范数与谱范数</a>    <ul>
      <li><a href="#核范数" id="markdown-toc-核范数">核范数</a></li>
      <li><a href="#谱范数" id="markdown-toc-谱范数">谱范数</a></li>
      <li><a href="#半定规划sdp" id="markdown-toc-半定规划sdp">半定规划（SDP）</a></li>
    </ul>
  </li>
  <li><a href="#多重比较" id="markdown-toc-多重比较">多重比较</a>    <ul>
      <li><a href="#什么都不做" id="markdown-toc-什么都不做">什么都不做</a></li>
      <li><a href="#bonferroni-correction" id="markdown-toc-bonferroni-correction">Bonferroni correction</a></li>
      <li><a href="#benjaminihochberg-procedure" id="markdown-toc-benjaminihochberg-procedure">Benjamini–Hochberg procedure</a></li>
      <li><a href="#prism-里的方法" id="markdown-toc-prism-里的方法">Prism 里的方法</a></li>
      <li><a href="#什么时候用什么" id="markdown-toc-什么时候用什么">什么时候用什么</a></li>
    </ul>
  </li>
  <li><a href="#svd" id="markdown-toc-svd">SVD</a>    <ul>
      <li><a href="#公式" id="markdown-toc-公式">公式</a></li>
      <li><a href="#svd-和特征向量特征值" id="markdown-toc-svd-和特征向量特征值">SVD 和特征向量/特征值</a></li>
      <li><a href="#奇异向量和-svd" id="markdown-toc-奇异向量和-svd">奇异向量和 SVD</a></li>
      <li><a href="#svd-中的小知识点" id="markdown-toc-svd-中的小知识点">SVD 中的小知识点</a></li>
    </ul>
  </li>
  <li><a href="#pca" id="markdown-toc-pca">PCA</a>    <ul>
      <li><a href="#pca-的目的" id="markdown-toc-pca-的目的">PCA 的目的</a></li>
      <li><a href="#找到-u-和-w" id="markdown-toc-找到-u-和-w">找到 U 和 W</a></li>
    </ul>
  </li>
  <li><a href="#向量微分算子" id="markdown-toc-向量微分算子">向量微分算子</a>    <ul>
      <li><a href="#散度" id="markdown-toc-散度">散度</a></li>
      <li><a href="#梯度" id="markdown-toc-梯度">梯度</a></li>
      <li><a href="#旋度" id="markdown-toc-旋度">旋度</a></li>
      <li><a href="#拉普拉斯算符" id="markdown-toc-拉普拉斯算符">拉普拉斯算符</a></li>
      <li><a href="#其他" id="markdown-toc-其他">其他</a></li>
    </ul>
  </li>
</ul>

<h2 id="马尔可夫链">马尔可夫链</h2>

<h3 id="定义">定义</h3>

<p>随机过程（很多随机变量的有序排列 ${ X_i }$）如果满足下面的性质，则被成为马尔可夫过程：</p>

\[\begin{aligned}
P_{ij} &amp;= \Pr\left\{X_{n+1}=j \vert X_n=i, X_{n-1}=i_{n-1}, \dots, X_0=i_0\right\} \\[1em]
&amp;= \Pr\left\{ X_{n+1} = j \vert X_n=i \right\}
\end{aligned}\]

<p>换言之，在时刻 $i$ 的状态 $X_i$ 确定了马尔可夫链的未来状态。</p>

<p>与马尔可夫链相关的问题中，最困难的部分通常是，</p>

<ol>
  <li>如何选择合适的状态空间；</li>
  <li>如何定义转移概率。</li>
</ol>

<h3 id="重要的矩阵">重要的矩阵</h3>

<p>Markov Chain 的性质由它的<strong>转移矩阵（Transition Matrix）</strong>描述，表示 $M$ 个状态之间转移的概率：</p>

\[\begin{aligned}
\mathbf{P} &amp;= P_{ij} \\[1em] &amp;= \begin{bmatrix}
p_{11} &amp; p_{12} &amp; \cdots &amp; p_{1M} \\
p_{21} &amp; p_{22} &amp; \cdots &amp; p_{2M} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
p_{M1} &amp; p_{M2} &amp; \cdots &amp; p_{MM}
\end{bmatrix}
\end{aligned}\]

<p>此外，我们也通过<strong>首次到达时间（first passage time）</strong>矩阵，或者<strong>击中时间（hitting time）</strong>矩阵 $\mathbf{T}$，来描述马尔可夫链的长期行为：</p>

\[\mathbf{T} = T_{ij} = \min(n: X_n = j \vert X_0 = i)\]

<p>每个元素 $T_{ij}$ 表示从状态 $i$ 到状态 $j$ 所需的最小步数 $n$。</p>

<h3 id="状态的分类">状态的分类</h3>

<p><strong>可达（accessible）</strong></p>

<ul>
  <li>存在一个正整数 $n$ 使得从状态 $i$ 到 $j$ 的转移概率大于零</li>
  <li>$\exists n: P_{ij}^n &gt; 0$</li>
  <li>$\Pr(T_{ij} &lt; \infty) &gt; 0$</li>
</ul>

<p><strong>相通（communicate）</strong></p>

<ul>
  <li>从 $i$ 到 $j$ 以及从 $j$ 到 $i$ 都可达</li>
  <li>$\forall j: \Pr(T_{ij} &lt; \infty) &gt; 0 \Rightarrow \Pr(T_{ji}) = 1$</li>
</ul>

<p><strong>常返（recurrent）</strong></p>

<ul>
  <li>如果从状态 $i$ 出发，最终以概率 1 返回到状态 $i$。</li>
  <li>$\Pr(T_{ii} &lt; \infty) = 1$</li>
  <li>$P_i = \lim_{n\rightarrow\infty} \Pr(X_n = i \vert X_0 = i) = 1$</li>
</ul>

<p><strong>暂态（transient）</strong></p>

<ul>
  <li>如果一个状态不是常返状态，那么它就是暂态状态。</li>
  <li>从暂态状态出发，有可能永远不返回到该状态。</li>
  <li>$P_i = \lim_{n\rightarrow\infty} \Pr(X_n = i \vert X_0 = i) &lt; 1$</li>
</ul>

<p><strong>吸收态（absorbing）</strong></p>

<ul>
  <li>不能离开的状态。</li>
  <li>$P_{ii} = 1$ and $P_{ij} = 0 \ \forall j \neq i$</li>
  <li>拥有吸收态的马尔可夫链，被成为吸收的。</li>
</ul>

<p><strong>例子</strong></p>

<svg width="323pt" height="109pt" viewBox="0.00 0.00 323.00 109.34" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink">
<g id="graph0" class="graph" transform="scale(1 1) rotate(0) translate(4 105.34)">
<title>%3</title>
<polygon fill="white" stroke="transparent" points="-4,4 -4,-105.34 319,-105.34 319,4 -4,4" />
<!-- 1 -->
<g id="node1" class="node">
<title>1</title>
<ellipse fill="none" stroke="black" cx="18" cy="-24.34" rx="18" ry="18" />
<text text-anchor="middle" x="18" y="-20.64" font-family="Times,serif" font-size="14.00">1</text>
</g>
<!-- 2 -->
<g id="node2" class="node">
<title>2</title>
<ellipse fill="none" stroke="black" cx="109" cy="-58.34" rx="18" ry="18" />
<text text-anchor="middle" x="109" y="-54.64" font-family="Times,serif" font-size="14.00">2</text>
</g>
<!-- 1&#45;&gt;2 -->
<g id="edge1" class="edge">
<title>1&#45;&gt;2</title>
<path fill="none" stroke="tomato" d="M35.32,-30.57C49.88,-36.13 71.26,-44.3 87.07,-50.34" />
<polygon fill="tomato" stroke="tomato" points="86.48,-51.99 91.78,-52.14 87.73,-48.72 86.48,-51.99" />
<text text-anchor="middle" x="63.5" y="-47.34" font-family="Fira Code" font-size="10.00" fill="tomato">0.5</text>
</g>
<!-- 4 -->
<g id="node3" class="node">
<title>4</title>
<ellipse fill="none" stroke="black" cx="297" cy="-31.34" rx="18" ry="18" />
<text text-anchor="middle" x="297" y="-27.64" font-family="Times,serif" font-size="14.00">4</text>
</g>
<!-- 1&#45;&gt;4 -->
<g id="edge2" class="edge">
<title>1&#45;&gt;4</title>
<path fill="none" stroke="tomato" d="M35.6,-19.64C70.65,-10.39 154.68,8.23 224,-4.34 242.17,-7.63 261.74,-15.25 275.87,-21.56" />
<polygon fill="tomato" stroke="tomato" points="275.18,-23.16 280.45,-23.64 276.62,-19.98 275.18,-23.16" />
<text text-anchor="middle" x="157.5" y="-3.34" font-family="Fira Code" font-size="10.00" fill="tomato">0.5</text>
</g>
<!-- 2&#45;&gt;1 -->
<g id="edge3" class="edge">
<title>2&#45;&gt;1</title>
<path fill="none" stroke="royalblue" d="M98.12,-43.99C91.91,-36.29 83.16,-27.54 73,-23.34 62.97,-19.18 50.96,-18.99 40.82,-20.03" />
<polygon fill="royalblue" stroke="royalblue" points="40.45,-18.31 35.71,-20.67 40.88,-21.78 40.45,-18.31" />
<text text-anchor="middle" x="63.5" y="-26.34" font-family="Fira Code" font-size="10.00" fill="royalblue">0.5</text>
</g>
<!-- 2&#45;&gt;4 -->
<g id="edge5" class="edge">
<title>2&#45;&gt;4</title>
<path fill="none" stroke="royalblue" d="M124.62,-67.43C146.53,-79.87 189.04,-99.33 224,-87.34 246.45,-79.63 267.3,-61.75 280.72,-48.25" />
<polygon fill="royalblue" stroke="royalblue" points="282.07,-49.37 284.3,-44.57 279.56,-46.93 282.07,-49.37" />
<text text-anchor="middle" x="206" y="-93.34" font-family="Fira Code" font-size="10.00" fill="royalblue">0.25</text>
</g>
<!-- 3 -->
<g id="node4" class="node">
<title>3</title>
<ellipse fill="none" stroke="black" cx="206" cy="-31.34" rx="18" ry="18" />
<text text-anchor="middle" x="206" y="-27.64" font-family="Times,serif" font-size="14.00">3</text>
</g>
<!-- 2&#45;&gt;3 -->
<g id="edge4" class="edge">
<title>2&#45;&gt;3</title>
<path fill="none" stroke="royalblue" d="M126.55,-53.63C142.25,-49.17 165.96,-42.43 183.2,-37.53" />
<polygon fill="royalblue" stroke="royalblue" points="183.99,-39.13 188.32,-36.08 183.04,-35.76 183.99,-39.13" />
<text text-anchor="middle" x="157.5" y="-50.34" font-family="Fira Code" font-size="10.00" fill="royalblue">0.25</text>
</g>
<!-- 4&#45;&gt;4 -->
<g id="edge9" class="edge">
<title>4&#45;&gt;4</title>
<path fill="none" stroke="crimson" d="M289.33,-47.63C287.48,-57.72 290.04,-67.34 297,-67.34 302.76,-67.34 305.51,-60.74 305.23,-52.74" />
<polygon fill="crimson" stroke="crimson" points="306.95,-52.4 304.67,-47.63 303.48,-52.79 306.95,-52.4" />
<text text-anchor="middle" x="297" y="-70.34" font-family="Fira Code" font-size="10.00" fill="crimson">1</text>
</g>
<!-- 3&#45;&gt;2 -->
<g id="edge6" class="edge">
<title>3&#45;&gt;2</title>
<path fill="none" stroke="seagreen" d="M188.41,-26.59C176.04,-23.91 158.96,-22.04 145,-27.34 137.48,-30.19 130.53,-35.48 124.85,-40.89" />
<polygon fill="seagreen" stroke="seagreen" points="123.41,-39.86 121.12,-44.63 125.89,-42.33 123.41,-39.86" />
<text text-anchor="middle" x="157.5" y="-30.34" font-family="Fira Code" font-size="10.00" fill="seagreen">0.4</text>
</g>
<!-- 3&#45;&gt;4 -->
<g id="edge7" class="edge">
<title>3&#45;&gt;4</title>
<path fill="none" stroke="seagreen" d="M224.16,-31.34C238.32,-31.34 258.51,-31.34 273.88,-31.34" />
<polygon fill="seagreen" stroke="seagreen" points="273.89,-33.09 278.89,-31.34 273.89,-29.59 273.89,-33.09" />
<text text-anchor="middle" x="251.5" y="-34.34" font-family="Fira Code" font-size="10.00" fill="seagreen">0.2</text>
</g>
<!-- 3&#45;&gt;3 -->
<g id="edge8" class="edge">
<title>3&#45;&gt;3</title>
<path fill="none" stroke="seagreen" d="M198.33,-47.63C196.48,-57.72 199.04,-67.34 206,-67.34 211.76,-67.34 214.51,-60.74 214.23,-52.74" />
<polygon fill="seagreen" stroke="seagreen" points="215.95,-52.4 213.67,-47.63 212.48,-52.79 215.95,-52.4" />
<text text-anchor="middle" x="206" y="-70.34" font-family="Fira Code" font-size="10.00" fill="seagreen">0.4</text>
</g>
</g>
</svg>

<ul>
  <li>从状态 1 开始，状态 4 可达；但是反过来不成立。</li>
  <li>状态 1 和 状态 3 相通。</li>
  <li>状态 4 是常返状态；其他是暂态。</li>
</ul>

<h3 id="定义-1">定义</h3>

<p>在 $ \mathbb{R}^K $ 中的<strong>单纯形</strong>是所有点 $ p $ 的集合，使得：</p>

<ul>
  <li>每个组件 $ p_k $ 都是非负的：对于所有 $ k $，都有 $ p_k \geq 0 $。</li>
  <li>组件的总和为一：$ \sum_{k=1}^{K} p_k = 1 $。</li>
</ul>

<p>数学上，这可以表示为：</p>

\[\Delta^{K-1} = \{ p \in \mathbb{R}^K : p_k \geq 0, \sum_{k=1}^{K} p_k = 1 \}\]

<h3 id="几何解释">几何解释</h3>

<ul>
  <li>在 $ \mathbb{R}^2 $（2D 空间）中，单纯形是一个线段。</li>
  <li>在 $ \mathbb{R}^3 $（3D 空间）中，它是一个三角形。</li>
  <li>在 $ \mathbb{R}^4 $ 中，它是一个四面体。</li>
  <li>以此类推…</li>
</ul>

<h3 id="与概率的关系">与概率的关系</h3>

<p>单纯形与概率分布有着密切的关系：</p>
<ul>
  <li>$ (K-1) $-维单纯形中的一个点对应于 $ K $ 个结果上的概率分布。</li>
  <li>例如，在 $ \mathbb{R}^3 $ 中，单纯形中的一个点可以表示三个事件上的概率分布。</li>
</ul>

<h3 id="单纯复合体">单纯复合体</h3>

<p><strong>单纯复合体</strong>是由顶点、线段、三角形及其 $ n $-维对应物组成的集合。它是单纯形概念的推广，可以表示由各种维度的单纯形填充的空间。</p>

<h2 id="核范数与谱范数">核范数与谱范数</h2>

<h3 id="核范数">核范数</h3>

<p>对于矩阵$A$，其核范数（Nuclear Norm 或者 Trace Norm，通常记作 $|A|_*$）是其奇异值的总和。</p>

<p><strong>使用Numpy的Python示例</strong>:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">compute_nuclear_norm</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="n">singular_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">svd</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">singular_values</span><span class="p">)</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">compute_nuclear_norm</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>核范数最小化的伪代码</strong>:</p>

<p>核范数可以作为矩阵秩在优化问题中的凸替代。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>function NuclearNormMinimization(A, mask):
    使用零初始化矩阵 X
    当未达到收敛条件:
        # 梯度步骤: 逼近已知条目
        基于A的条目（mask为True的地方）更新X
        # 收缩步骤: 促进低秩
        对X应用SVD并缩小奇异值
    return X
</code></pre></div></div>

<h3 id="谱范数">谱范数</h3>

<p>对于矩阵 $A$，其谱范数（spectral norm, 通常记作 $|A|_2$）是其最大的奇异值。</p>

<p>使用 Numpy 的 Python 示例:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">numpy</span> <span class="k">as</span> <span class="n">np</span>

<span class="k">def</span> <span class="nf">compute_spectral_norm</span><span class="p">(</span><span class="n">A</span><span class="p">):</span>
    <span class="n">singular_values</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">linalg</span><span class="p">.</span><span class="nf">svd</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">compute_uv</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">singular_values</span><span class="p">)</span>

<span class="n">A</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">],</span> <span class="p">[</span><span class="mi">5</span><span class="p">,</span> <span class="mi">6</span><span class="p">]])</span>
<span class="nf">print</span><span class="p">(</span><span class="nf">compute_spectral_norm</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>
</code></pre></div></div>

<p><strong>带有谱范数约束的优化的伪代码</strong>:</p>

<p>谱范数可以用作优化问题中的约束，以界定矩阵的“大小”或“效果”。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>function SpectralNormConstrainedOptimization(A, gamma):
    使用零初始化矩阵 X
    当未达到收敛条件:
        # 梯度步骤: 逼近目标
        更新 X
        # 投影步骤: 确保谱范数约束
        如果 SpectralNorm(X) &gt; gamma:
            缩放X以满足谱范数 &lt;= gamma
    return X
</code></pre></div></div>

<h3 id="半定规划sdp">半定规划（SDP）</h3>

<p>核范数和谱范数都可以整合到形式化为 SDP 的优化问题中。</p>

<p><strong>带有核范数最小化的SDP的伪代码</strong>:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>function SDPWithNuclearNorm(A):
    初始化半定矩阵 Y
    当未达到收敛条件:
        # 梯度步骤: 逼近数据拟合
        基于A更新Y
        # 收缩步骤: 促进结构
        对Y应用某些操作以鼓励所需结构
    return Y
</code></pre></div></div>

<h2 id="多重比较">多重比较</h2>

<p>统计中，$P$ 值的含义是 null 假设为真的概率是 $1-P$。</p>

<p>如果我们以 $P=0.05$ 来判断 null 假设的成立与否，那么我们的「每一次」判断都有 5% 的概率出错——我们测试出来的每一个「统计学显著的差异」都有 5% 的出错几率。</p>

<p>如果我们在一个项目中同时做了 $n$ 个统计学检验。那么我们「所有检验结果都为真」的概率是 $(1-P)^n$ ——在 $n$ 很大的时候，<strong>我们几乎注定会出错</strong>！</p>

<p>下面介绍的两个方法都用与减轻 multiple comparison 的副作用。它们的大概作用都是「降低 false positive」的几率。不过这也会同时增加我们的 false negative 的概率。</p>

<p>我们是否要「主动降低 false positive 概率」呢？这个取决于「代价」，需要我们自己定夺。</p>

<h3 id="什么都不做">什么都不做</h3>

<p>很多人建议，什么都不要做：把实验中用到的所有分析结果，以及 p 值都报道出来。这让阅读实验报告的人能够理解，有 5% 的结论可能是错的。</p>

<p>如果实验是预先设计，并且严格按照计划执行和分析的，我们通常也不需要做额外的 correction。「按照计划」是指我们没有「为了降低 p 值」而增加样本数目。</p>

<h3 id="bonferroni-correction">Bonferroni correction</h3>

<blockquote>
  <p>这个小节参考了<a href="http://www.biostathandbook.com/multiplecomparisons.html">这个网站</a></p>
</blockquote>

<p>Bonferroni Correction 的要点：以 $P^\prime$ 作为新的 $P$ 值，$P^\prime = P / n$</p>

<p>这个方法的缺点是，如果我们有很多统计学分析（$n = 1000$)，那么我们的 $P^\prime$ 会很小：这回让我们把很多「不符合」null 假设的比较设定为 null 假设。换言之，我们会产生很多的 false negative。</p>

<p>此外，我们还面临着「历史」的问题：如果我们在 10 年前分析了 100 个样品，我们「现在」的统计分析是否需要将那 100 个分析算在我们的 $n$ 里？——这个问题没有统一的答案。</p>

<h3 id="benjaminihochberg-procedure">Benjamini–Hochberg procedure</h3>

<blockquote>
  <p>这个 小节 参考了<a href="http://www.biostathandbook.com/multiplecomparisons.html">这个网站</a></p>
</blockquote>

<p>在 Bonferroni Correction 外，另外一个方法是控制 false discovery rate。它的具体做法是</p>

<ol>
  <li>计算出所有统计分析的 P 值</li>
  <li>将 P 值由小到大「排序」。我们假设某一个实验的排序结果为 $i$ ($i=1$ 表示 $P$ 值最小的结果)。</li>
  <li>我们人工选择一个 $Q$ 值，这个值被称为 false discovery rate。</li>
  <li>对于「每一个」统计分析结果，我们它对应的 $(i/m)Q$</li>
</ol>

<p>上面的结果之后，我们会得到两列数据</p>

<table>
  <thead>
    <tr>
      <th>实验序号</th>
      <th>P 值</th>
      <th>(i/m)Q</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>0.001</td>
      <td>0.01</td>
    </tr>
    <tr>
      <td>2</td>
      <td>0.008</td>
      <td>0.02</td>
    </tr>
    <tr>
      <td>3</td>
      <td>0.039</td>
      <td>0.03</td>
    </tr>
    <tr>
      <td>4</td>
      <td>0.041</td>
      <td>0.04</td>
    </tr>
    <tr>
      <td><strong>5</strong></td>
      <td><strong>0.042</strong></td>
      <td><strong>0.05</strong></td>
    </tr>
    <tr>
      <td>6</td>
      <td>0.060</td>
      <td>0.06</td>
    </tr>
    <tr>
      <td>7</td>
      <td>0.074</td>
      <td>0.07</td>
    </tr>
    <tr>
      <td>…</td>
      <td>…</td>
      <td>…</td>
    </tr>
  </tbody>
</table>

<p>我们取这样的 $P$ 值作为差异显著的判断标准：最大的，小于 $(i/m)Q$ 的 $P$。</p>

<p>对应上面的表格，我们会选择「第五行」数据对应的 $P$ 值，0.042，作为我们的判断标准。</p>

<p>有的时候，我们把 $P (m / i)$ 称为 Benjamini-Hochberg adjusted P value。只要它小于我们设定的 $Q$，我们就认为这个结果是显著的。</p>

<p>这个方法会导致一个奇怪的结论：如果我们分析了 1000 个结果，其中最大的 $P$ 值是 0.24，最小的 $P$ 值是 0.1。按照这个小节的方法，我们会拒绝所有的 null 假设（认为我们的样品有统计学显著性），即使它们的 $P$ 值都大于 0.05。</p>

<p>这其实也是合理的：如果我们想要接受 1000 个 null 假设，我们最大的 $P$ 值应该挺大的——在 0.9 左右。</p>

<p>与 Bonferroni 方法相比，这个方法不会产生「当统计检验数目 $n$ 很大时，接受所有的 null 假设」的情况。</p>

<p>对于 $Q$ 值，0.1 或者 0.2 是很严格的标准了——有时候有人会用 0.05，这可能是此人混淆了 $Q$ 值与 $P$ 值。</p>

<p>注意：这个方法默认所有的测试都是「相互独立」的。如果我们是比较一个 control 组和其他很多实验组，这个方法是不适用的。举例而言，如果 control &gt; A &amp;&amp; A &gt; B 那么 control 一定大于 B。</p>

<h3 id="prism-里的方法">Prism 里的方法</h3>

<p>在软件 Prism 里，我们可以选择 Bonferroni, Tukey, Dunnett 或者 Dunn 等方法。它们有类似之处：都会报告一个 adjusted P 值。我们得到的 adjusted P 的含义是</p>

<blockquote>
  <p>对我们的「某一个」统计检验，我们寻找一个「最小的」P 值，把这个值应用在我们多重比较 (family) 里，我们能认为「某一个」检验不符合 null 假设。</p>
</blockquote>

<h3 id="什么时候用什么">什么时候用什么</h3>

<ul>
  <li>比较一个 control 和其他样本的均值： Dunnett’s method</li>
  <li>比较所有样本的均值：Tukey method</li>
  <li>比较我们主观选择的一些样本：Bonferroni / Šídák method</li>
</ul>

<h2 id="svd">SVD</h2>

<h3 id="公式">公式</h3>

<p>SVD 一般是这样写的</p>

\[\begin{align}
\bf{A_{m \times n}} = 
\bf{U_{m \times n}} \;
\Sigma_{n \times n} \;
\bf{V_{n \times n}}
\end{align}\]

<p>其中，$\bf{U}$ 和 $\bf{V}$ 都是酉矩阵，它们表示旋转操作，同时它们的行/列都是标准正交基（基向量的长度为 1，两两正交）。</p>

<h3 id="svd-和特征向量特征值">SVD 和特征向量/特征值</h3>

<p>我们想要得到的其实是类似于「特征值」的东西，对于方阵 $\bf{X}$，它的特征值 $\upsilon$ 满足</p>

\[\mathbf{X} \mathbf{\upsilon} = \lambda \mathbf{\upsilon}\]

<p>对于不是方阵的 $A$，我们能找到两个类似于特征值的东西（$\bf{v}$, $\bf{u}$），叫做「一个奇异向量」。对应的数值 $\sigma$ 叫做「一个奇异值」：</p>

\[\begin{align}
\bf{A^T} u &amp;= \sigma \bf{v} &amp; \bf{u}\text{: 左奇异向量} \\
\bf{A} v &amp;= \sigma \bf{u} &amp; \bf{v} \text{: 右奇异向量}
\end{align}\]

<p>如果我们给上面的公式左右分别左乘上 $\bf{A}$ 和 $\bf{A^T}$，我们得到</p>

\[\begin{aligned}
\bf{AA^T} u &amp;= \sigma \bf{Av}\\
&amp;= \sigma^2 \bf{u}
&amp;(\bf{u} \text{ 是} \bf{AA^T} \text{ 的特征值})\\[1em]
\bf{A^TA} v &amp;= \sigma \bf{u}\\
&amp;= \sigma^2 \bf{v}
&amp; (\bf{v} \text{ 是} \bf{A^TA} \text{ 的特征值})\\
\end{aligned}\]

<p>在这里，我知考虑了实数矩阵——如果考虑复数的话，我需要计算 $\bf{A}$ 的共轭转置 $\bf{A^\ast}$。</p>

<h3 id="奇异向量和-svd">奇异向量和 SVD</h3>

<p>公式 (1) 中的 $\bf{U}$ 和 $\bf{V}$ 是所有包含了所有左/右奇异向量的矢量空间的标准正交基。而公式 (1) 中的 $\Sigma$ 则表示了每个奇异向量对应的奇异值。</p>

<p>让我们重新审视公式 (1)</p>

\[\begin{align}
\bf{A} &amp;= \bf{U} \bf{\Sigma} \bf{V^T}
\end{align}\]

<p>我们可以把它稍微改写成</p>

\[\begin{aligned}
\bf{A} = \bf{U} \bf{\Sigma} \bf{V^T} \bf{I}
\end{aligned}\]

<p>其中，$\bf{I}$ 是标准正交基。上面的公式告诉了我们怎么把 $\bf{I}$ 变成 $\bf{A}$：我们首先把 $\bf{I}$ 映射到右奇异基向量（旋转）；按照奇异值大小缩放各个基向量；再把结果映射到左奇异基向量（旋转）。</p>

<h3 id="svd-中的小知识点">SVD 中的小知识点</h3>

<ul>
  <li>$\Sigma$ 中的奇异值的「有序集合」被称为矩阵的「谱」spectrum。</li>
  <li>奇异值之间的差影响着解的稳定性</li>
  <li>最大和最小的奇异值绝对值之间的比率（条件数）影响着一个迭代求解器找到矩阵解的速度。</li>
</ul>

<h2 id="pca">PCA</h2>

<h3 id="pca-的目的">PCA 的目的</h3>

<p>我们使用 PCA 来压缩数据。假设我们有 $m$ 个属于 $\mathbb{R}^d$ 列向量：$\bf{x_1}, \dots \bf{x_m}$，我们用一个矩阵 $\mathbf{x}\in\mathbb{R}^{d, m}$ 来描述所有的数据点。我们希望找到一个矩阵 $\mathbf{W} \in \mathbb{R}^{d, n}$，来将 $\bf{x}$ 映射到低维空间：</p>

\[\begin{align}
\mathbf{y} &amp;= \mathbf{Wx} &amp; (\ \mathbf{y} \in \mathbb{R}^n, n &lt; d)
\end{align}\]

<p>同样，我们也需要另一个矩阵 $\mathbf{U} \in \mathbb{R}^{n, d}$ 来将 $\bf{y}$ 还原为一个「同原始 $\bf{x}$ 接近」的 $\bf{\tilde{x}}$。我们通过让两者「尽量接近」来确定最好的 $\bf{U}$ 和 $\bf{W}$：</p>

\[\begin{align}
\underset{
\mathbf{W}\in\mathbb{R}^{n, d},\
\mathbf{U}\in\mathbb{R}^{d, n}
}{\operatorname{argmin}}
\sum_{i=1}^{m}{\left\|
\mathbf{x}_i - \mathbf{UWx}_i
\right\|^2}
\end{align}\]

<h3 id="找到-u-和-w">找到 U 和 W</h3>

<p>我们首先可以证明，我们想要的，符合公式 (6) 的， $\bf{U}$ 与 $\bf{W}$ 满足下面的性质（其实就是说，这两个都是旋转矩阵）</p>

<ol>
  <li>$\bf{U}$ 的列向量相互正交，即 $\bf U^\top U=I$</li>
  <li>$\bf W = U^\top$</li>
</ol>

<p>根据上面的辅助定理，我们可以把公式 (6) 重新写成</p>

\[\begin{align}
\underset{
\mathbf{W}\in\mathbb{R}^{n, d},\
\mathbf{U}\in\mathbb{R}^{d, n}
}{\operatorname{argmin}}
\sum_{i=1}^{m}{\left\|
\mathbf{x}_i - \mathbf{UU^\top x}_i
\right\|^2}
\end{align}\]

<p>我们可以把需要 minimise 的部分写成</p>

\[\begin{aligned}\left\|\mathbf{x}-U U^{\top} \mathbf{x}\right\|^{2} &amp;=\|\mathbf{x}\|^{2}-2 \mathbf{x}^{\top} U U^{\top} \mathbf{x}+\mathbf{x}^{\top} U U^{\top} U U^{\top} \mathbf{x} \\ &amp;=\|\mathbf{x}\|^{2}-\mathbf{x}^{\top} U U^{\top} \mathbf{x} \\ &amp;=\|\mathbf{x}\|^{2}-\operatorname{trace}\left(U^{\top} \mathbf{x} \mathbf{x}^{\top} U\right) \end{aligned}\]

<p>其中，$\text{trace}$ 的意思是对对角项求和，是一个线性算符，这让我们可以最终把公式 (6) 写成</p>

\[\underset{U \in \mathbb{R}^{d, n} : U^{\top} U=I}{\operatorname{argmax}} \operatorname{trace}\left(U^{\top} \sum_{i=1}^{m} \mathbf{x}_{i} \mathbf{x}_{i}^{\top} U\right)\]

<h2 id="向量微分算子">向量微分算子</h2>

<p>Del 算子或稱 Nabla 算子，在中文中也叫向量微分算子、劈形算子、倒三角算子，符号为 $\nabla$。</p>

<p>Nabla 主要的作用就是用来简写「散度」、「梯度」和「旋度」。我们可以把它看成一个向量。</p>

<p>三维空间中</p>

\[\nabla=\vec{e}_{x} \frac{\partial}{\partial x}+\vec{e}_{y} \frac{\partial}{\partial y}+\vec{e}_{z} \frac{\partial}{\partial z}=\left(\frac{\partial}{\partial x}, \frac{\partial}{\partial y}, \frac{\partial}{\partial z}\right)\]

<p>n 维空间中</p>

\[\nabla=\sum_{i=1}^{n} \vec{e}_{i} \frac{\partial}{\partial x_{i}}=\left(\frac{\partial}{\partial x_{1}}, \cdots, \frac{\partial}{\partial x_{n}}\right)\]

<h3 id="散度">散度</h3>

<ul>
  <li>
    <p>记号： $\nabla \cdot \mathbf{A}$</p>
  </li>
  <li>
    <p>维度： $\mathbb{R}^n \rightarrow \mathbb{R}$</p>
  </li>
  <li>
    <p>意义：散度描述的是向量场里一个点是汇聚点还是发源点，形象地说，就是这包含这一点的一个微小体元中的向量是“向外”居多还是“向内”居多。</p>
  </li>
  <li>
    <p>分量表示
\(\begin{aligned}
\nabla \cdot \mathbf{A} &amp;= \operatorname{div} \mathbf{A} \\
&amp;= \left(
	\frac{\partial}{\partial x},
	\frac{\partial}{\partial y},
	\frac{\partial}{\partial z}
\right) \cdot (A_x, A_y, A_z)\\[0.5em]
&amp;= \frac{\partial A_{x}}{\partial x}+\frac{\partial A_{y}}{\partial y}+\frac{\partial A_{z}}{\partial z}
\end{aligned}\)</p>
  </li>
</ul>

<h3 id="梯度">梯度</h3>

<ul>
  <li>
    <p>记号：$\nabla \mathbf{A}$</p>
  </li>
  <li>
    <p>维度：$\mathbb{R}^n \rightarrow \mathbb{R}^n$</p>
  </li>
  <li>意义：
    <ul>
      <li>方向：多元函数的某一个点上，最大增长的「方向」</li>
      <li>大小：多元函数的某一个点上，最大增长的「增长率」</li>
    </ul>
  </li>
  <li>分量表示
  \(\begin{aligned}
  \nabla \mathbf{A}
  &amp;= \text{grad}\,\mathbf{A} = \mathbf{J}_\mathbf{A} \\[0.5em]
  &amp;= \left(
  	\frac{\partial \textbf{A}}{\partial x},
  	\frac{\partial \textbf{A}}{\partial y},
  	\frac{\partial \textbf{A}}{\partial z}
  \right)\\[0.5em]
  &amp;= \left( \frac{\partial A_i}{ \partial x_j} \right)_{ij}
  \end{aligned}\)</li>
</ul>

<h3 id="旋度">旋度</h3>

<ul>
  <li>记号：$\nabla \times \mathbf{A}$</li>
  <li>维度：$\mathbb{R}^n \rightarrow \mathbb{R}^n$</li>
  <li>意义：在向量场每个点上，点的<strong>旋度</strong>表示为一个<strong>向量</strong>。这个向量的特性（长度和方向）刻画了在这个点上的旋转。</li>
  <li>分量表示:</li>
</ul>

\[\begin{aligned}
\operatorname{curl} \mathbf{A}
&amp;=\boldsymbol{\nabla} \times \mathbf{A}\\[1em]
&amp;=\left(\frac{\partial A_{z}}{\partial y}-\frac{\partial A_{y}}{\partial z}\right) \mathbf{i}+\left(\frac{\partial A_{x}}{\partial z}-\frac{\partial A_{z}}{\partial x}\right) \mathbf{j}+\left(\frac{\partial A_{y}}{\partial x}-\frac{\partial A_{x}}{\partial y}\right) \mathbf{k}\\[1em]
&amp;=\left|\begin{array}{ccc}{\mathbf{i}} &amp; {\mathbf{j}} &amp; {\mathbf{k}} \\ {\frac{\partial}{\partial x}} &amp; {\frac{\partial}{\partial y}} &amp; {\frac{\partial}{\partial z}} \\ {A_{x}} &amp; {A_{y}} &amp; {A_{z}}\end{array}\right|
\end{aligned}\]

<h3 id="拉普拉斯算符">拉普拉斯算符</h3>

<p>矢量拉普拉斯算符（vector Laplacian）作用在 <strong>矢量</strong>场 上，返回一个<strong>矢量</strong>。类似地，标量拉普拉斯算符（scalar Laplacian）作用在 <strong>标量</strong>场 上，返回一个<strong>标</strong>量。</p>

<p>作用在 矢量场 $\mathbf{A}$ 的 矢量拉普拉斯算符 写成 $\nabla^2$，定义为
\(\nabla^2 = \nabla(\nabla \cdot \mathbf{A})
         - \nabla\times(\nabla \times \mathbf{A})\)
在 笛卡尔坐标系 下，上面的定义可以被（极大地）简化为
\(\nabla^2\mathbf{A} = \left( 
\nabla^2 A_x, \nabla^2 A_y, \nabla^2 A_z
\right)\)
其中，$A_x， A_y, A_z$ 是矢量场 $\mathbf{A}$ 的分量</p>

<h3 id="其他">其他</h3>

\[\begin{aligned}
(\mathbf{A} \cdot \nabla)  \mathbf{B} &amp;= \left(
	A_i \frac{\partial}{\partial x_i}
\right) B_j \\
&amp;= A_i \left(
	\frac{
		\partial B_{\color{tomato}{j}}
	}{
		\partial x_{\color{teal}{i}}
	}
\right) \\[1em]
\mathbf{A} \cdot \nabla \mathbf{B} 
&amp;= \mathbf{A} \cdot \mathbf{J}_\mathbf{B} \\
&amp;= A_i \left(
	\frac{
		\partial B_{\color{teal}{i}}
	}{
		\partial x_{\color{tomato}{j}}
	}
\right)
\end{aligned}\]

<p><a href="https://en.wikipedia.org/wiki/Vector_calculus_identities">更多矢量微积分的记号</a></p>


</div>


    </div>

  </body>

</html>
