[
  {
    "id": "/code/2020/04/22/python.html",
    "title": "Python",
    "content": "The provided markdown post discusses the usage of the Sphinx documentation generator to build documentation for code. The author describes the process of using Sphinx to generate HTML webpages from text files, similar to using Jekyll. They provide a step-by-step guide for using Sphinx, including writing docstrings in code, generating the rst documentation for each package, and adding modules to the index. The author also mentions useful extensions for Sphinx, such as autodoc for generating documentation from code, napoleon for understanding Google-style docstrings, intersphinx for adding links to external documentation, and mathjax for rendering LaTeX equations in the documentation. The post also explains how to render code blocks and formulas in webpages using Sphinx. Additionally, the author discusses the use of the pybind11 library to accelerate Python code by rewriting slow parts in C++ and directly referencing them in Python scripts. They provide installation instructions and examples of how to use pybind11 to create Python modules from C++ code. The author also explains the benefits of using pybind11 to handle data conversions between Python and C++. Lastly, the post introduces the struct module for working with binary data in Python, the os module for interacting with the operating system, the subprocess module for running and communicating with external processes, the itertools module for working with iterators and generators, and the sqlite3 module for working with SQLite databases. The post provides examples and explanations of how to use these modules to perform various tasks. The author also mentions some basic concepts such as decorators, the usage of @ symbol to apply decorators to functions and classes, and the usage of dynamic typing in Python. Overall, the keyword summary for this post includes Sphinx, documentation, autodoc, napoleon, intersphinx, mathjax, pybind11, C++, eigen, structs, byte order, little-endian, big-endian, memory alignment, os, subprocess, iterators, itertools, databases, and sqlite3.Parameter Passing  In Python, the behavior of functions differs when handling immutable and mutable objects. For immutable objects, such as integers, the value of the object passed to a function will not be modified. In the example provided, the function `f` receives an integer `a` as a parameter, but changing the value of `x` inside the function does not affect the original value of `a`. However, for mutable objects, such as lists, passing the object to a function may change the original object. In the example provided, the function `f` modifies the value of the first element of the list `b` to 100, and this change is reflected in the original list. It is important to be careful when passing mutable objects to avoid unexpected modifications.   Closure  In Python, a closure is a function that references variables from its outer scope and is returned as a result. In the example provided, the function `line_conf` returns a function `line` that takes a variable `x` and returns the result of `a * x + b`. The variable `a` and `b` are stored in the “environment variables” of the closure and can be accessed when calling the returned function. The environment variables of a closure are stored in the `__closure__` attribute of the function object.  Context: with  Starting from Python 2.5, the “with” statement provides a syntax for defining the scope of an object and triggering specific operations upon entering or exiting this scope, such as allocating or releasing the object. The syntax is as follows: `with...as...`. For example, when using the `with` statement with a file object, the file is automatically closed when exiting the scope. This ensures that the file is properly closed and avoids resource leaks. The `with` statement works by calling the `__enter__()` method of the object upon entering the block, and the `__exit__()` method upon exiting the block. For file objects, the `__exit__()` method calls the `self.close()` method, so there is no need to manually close the file.  Serialization: pickle  In Python, all variables or functions are objects that are stored in memory, but they can be serialized (or pickled) and stored on disk. Serialized objects require the definition of their class to be able to recreate them. Built-in objects like integers or lists can be directly recreated, but for user-defined objects, the class definition needs to be provided. The `pickle` module provides functions to serialize and deserialize objects. The `pickle.dumps()` function serializes an object and returns a string representation of the object, which can be saved to a file. The `pickle.dump()` function saves an object directly to a file. To load a serialized object, the `pickle.load()` function is used.  Memory  In Python, variables are references to objects in memory. For example, when assigning `a = 1`, `a` is a reference to the integer object `1`. The `id()` function can be used to get the memory address of an object. Python optimizes commonly used small integers and short strings, so multiple references to the same object may have the same memory address. However, for larger numbers or strings, each reference may have a different memory address. Objects in Python also maintain a reference count, which represents how many references are pointing to the object. The `sys.getrefcount()` function can be used to get the reference count, but it returns a count that is one higher due to its own reference. Containers like lists or dictionaries hold references to other objects instead of the objects themselves. Objects can have circular references, forming a reference cycle. Garbage collection in Python automatically cleans up objects that are no longer in use.  Networking  To understand Python server frameworks like Django, Twisted, and web.py, it is important to understand the foundation of network communication: sockets. Sockets provide network-based communication between processes. By mastering sockets, developers can effectively use and design server frameworks. In Python, all things are objects, including classes. These classes have their own type, which is itself a metaclass called type. Metaclasses are advanced OOP techniques that enable metaprogramming. While most Python programs do not require metaclasses, they can be used for advanced manipulation. By understanding metaclasses and their usage, developers can have a deeper understanding of Python internals.  Metaprogramming  Metaprogramming is an advanced OOP technique that allows a program to understand or modify itself. In Python, classes are objects themselves and have a type, which is usually type. By defining custom metaclasses, it is possible to modify the behavior of a class creation process. The metaclass is defined by inheriting from type and implementing the `__new__()` method, which returns the newly created class. Custom metaclasses can modify the attributes and behavior of classes, allowing for advanced customization and behavior modification during class creation. While metaprogramming can be powerful, it is not commonly used in everyday programming and should be approached with caution.  Avoid Using Metaclasses  While metaclasses provide advanced capabilities for manipulating the class creation process, it is generally recommended to avoid using metaclasses unless there is a specific need. The majority of Python programs do not require metaclasses, and the complexity and potential for confusion make them unnecessary for most scenarios. In many cases, simpler techniques such as inheritance or decorators can achieve the desired results without the added complexity of metaclasses. It is important to weigh the benefits and drawbacks of using metaclasses and consider alternative approaches before deciding to use them.  The 10 most important keywords in the content are: parameter passing, closure, with, pickle, memory, networking, metaprogramming, metaclass, avoid, custom.",
    "sha256": "6fa0e31f93d4eac94cdd6add3b3d0445a19177f7574b44631c125f0a56afaf2c"
  },
  {
    "id": "/code/2022/04/02/nlp-deep-learning.html",
    "title": "Tensorflow Applications",
    "content": "In this markdown post, the author discusses two topics: Modelling Time Series and Modelling Natural Language.   For Modelling Time Series, the author begins with an introduction to time series, which are ordered sequences of equally spaced values over time. The author then explains that machine learning can be used for various time series applications, including predicting the future, retracing the past, detecting anomalies, and finding patterns. The author describes the typical patterns found in time series, such as trends, seasonality, white noise, and auto-correlation. They also discuss non-stationary time series, which may not follow the typical patterns.   Next, the author explains how to prepare data for time series modelling using the tf.data.Dataset class in TensorFlow. They provide code examples for creating datasets and manipulating them using methods like window, drop_remainder, and flat_map. They also demonstrate how to prepare training data by splitting the features and labels from the dataset using the map method. The author shows how to shuffle the data and batch it for training.  The author then introduces different models for time series forecasting, including baseline models like naive forecasting, moving average, and differencing. They also explain how to use linear regression, MLP (multi-layer perceptron), simple RNN (recurrent neural network), and LSTM (long short term memory) models for time series modelling. They provide code examples for constructing each model using the Sequential API in TensorFlow.  Finally, the author discusses how to evaluate the performance of time series predictions using various metrics like mean squared error (mse), root mean squared error (rmse), mean absolute error (mae), and mean absolute percentage error (mape).  For Modelling Natural Language, the author starts with an introduction to natural language processing (NLP), which involves extracting information from languages. The author gives an example of predicting the sentiment of movie comments. They describe the tokenization process, which involves converting text to numbers, and explain how to handle out-of-vocabulary words and pad sequences to a fixed length. The author provides code examples using the Tokenizer and pad_sequences classes in TensorFlow.  Next, the author discusses learning word embeddings, which are vector representations of words that capture their meanings. They explain how to train an embedding network using a predictive model and show the code for constructing such a model using the Embedding layer in TensorFlow.  The author then explains how to model the sequence of words using LSTM and 1D CNN models. They provide code examples for constructing these models using the Sequential API in TensorFlow.  Finally, the author demonstrates how to generate new text using a trained network. They provide code examples for training a network to predict the next word based on existing texts and for using the trained network to generate text.  The 10 most important keywords are: time series, modelling, TensorFlow, dataset, window, LSTM, NLP, tokenization, embedding, and sequence modelling.",
    "sha256": "1ea9c7393b17cefb7c006b68de2b0912f48849eecdddbefd4d312778fe14d597"
  },
  {
    "id": "/sci/2022/05/07/quantitative-model.html",
    "title": "Quantitative Modeling",
    "content": "This markdown post is a note on the fundamentals of quantitative modeling. It covers various topics related to modeling, including the purpose of modeling, different types of models, mathematical functions used in modeling, how models are used for prediction and optimization, the benefits of modeling, key steps in modeling, and a vocabulary for modeling. It also focuses on specific topics such as linear models, probabilistic models, regression models, and logistic regression. In the section on linear models, it discusses the concepts of constant proportionate growth, present and future value, continuous compounding, and optimization using calculus. In the section on probabilistic models, it covers random variables, probability distributions, regression models, probability trees, Monte Carlo simulation, and Markov models. The note also provides information on common probability distributions and the empirical rule for normal distribution. The section on regression models introduces the concepts of single regression and multiple regression, as well as logistic regression for discrete variables. Overall, the note provides a comprehensive overview of the fundamentals of quantitative modeling. The 10 most important keywords are: modeling, models, quantitative, assumptions, equations, optimization, probability, regression, variables, and distributions.",
    "sha256": "ca55e725060cc1b451b2d1129b6d6c5ac5ed397dfad2ab236d069666a9f77f45"
  },
  {
    "id": "/code/2020/01/11/C.html",
    "title": "C 语言",
    "content": "The provided content discusses various concepts related to C programming. It starts with an introduction to the creation process of C programs, including editing, compiling, linking, and execution. The structure of C programs is explained, including the preprocessor directives and functions, with the main() function being the entry point of every C program. The content also covers constants and variables, including different types of constants (integer, real, and character) and variable naming conventions. Different data types, such as integers and floats, are explained in detail, including their size, range, and storage representation. The content also covers characters and strings, including ASCII codes, string manipulation functions, and string input/output formatting. Operators in C, such as arithmetic, increment/decrement, and mixed data types operations, are discussed. The content also covers sequential structure design, including while and for loops, nested loops, and loop termination. Arrays, including one-dimensional and two-dimensional arrays, as well as strings as character arrays, are explained. Pointers, including basic usage, operators, pointer types, and applications, such as modifying function arguments and accessing array elements using pointers, are covered. File operations, including opening, closing, writing, and reading files, are explained.  Keywords: C programming, editing, compiling, linking, execution, structure, preprocessor directives, functions, main() function, constants, variables, data types, integers, floats, characters, strings, operators, sequential structure, loops, arrays, pointers, file operations",
    "sha256": "9b7a8ba0cdc766c920628da4404e289b66b110e2333aaa3b770de7efb516efaa"
  },
  {
    "id": "/code/2017/07/22/javascript.html",
    "title": "Javascript",
    "content": "JavaScript是一种脚本语言，主要用于为HTML网页增加动态功能。可以将JavaScript代码放在HTML页面的`<script>`标签中，或者单独存储在一个.js文件中并通过`<script src=file.js></script>`引用。JavaScript中的变量可以使用`var`关键字进行声明，也可以直接使用变量名进行赋值。JavaScript可以处理多种数据类型，包括字符串、数字、布尔值和数组。字符串可以使用单引号或双引号括起来，数字可以带小数点或不带小数点，布尔值只有true和false两种取值。数组可以用于存储多个变量。  JavaScript提供了多种运算符，包括算术、比较和逻辑运算符。算术运算符用于执行数学运算，比较运算符用于比较两个值的大小关系，逻辑运算符用于评估多个表达式的真假值。JavaScript还支持条件语句、循环语句和函数。条件语句例如if/else判断和switch/case结构，循环语句例如for循环和while循环，函数可以定义和调用以实现特定功能。  JavaScript是基于对象的语言，事件是JavaScript程序中的重要部分。事件是鼠标或热键的动作，每个事件都会引发一个事件处理程序的动作。常见的事件包括单击、双击、获得焦点、失去焦点、鼠标移动等。可以通过在HTML标签中添加相应的事件处理程序来捕获和处理事件。  JavaScript的数据都可以视为对象，每个对象都有属性和方法。属性是变量，表示对象的特定性质，方法是函数，实现特定的功能。可以通过关键词function定义函数，函数可以有多个参数，可以在函数体内部使用各种语句实现复杂的功能。需要注意的是，如果多次使用function命令重复声明同一个函数，后面的声明会覆盖前面的声明。  DOM是文档对象模型的简称，它把HTML和XML文档解析成一系列的节点，通过树状结构表示文档的结构。可以通过ID、名字、标签名等属性选取元素，通过getAttribute和setAttribute方法获取和设置元素的属性，通过parentNode方法获取元素的父节点，通过createElement和appendChild方法创建和插入节点，通过removeChild方法删除节点。  其中最重要的10个关键字为：JavaScript、HTML、动态功能、变量、数据类型、运算符、逻辑控制语句、函数、事件、DOM。",
    "sha256": "147ab4c7f9f4c47529ab7f285570763b491baa38392addf2fbd8c38370615926"
  },
  {
    "id": "/science/2020/07/03/stochastic-process.html",
    "title": "随机过程",
    "content": "This is a summary of a markdown post about random processes and counting processes. The post introduces the concept of random processes, which are collections of random variables indexed by time. There are two types of random processes: continuous-time random processes, which have a continuous time index, and discrete-time random processes, which have a discrete time index. The post discusses various concepts related to random processes, such as the cumulative distribution function (CDF), mean function, autocorrelation function (ACF), and cross-correlation function. It also introduces the concept of stationary processes, which are random processes whose statistical properties do not change over time. The post further discusses the properties of stationary processes, such as the constant mean function and the time-invariant ACF. It then introduces cyclostationary processes, which are stationary processes with periodic properties. The post also covers topics such as random variable calculus, Gaussian random processes, counting processes, and the Poisson process. The Poisson process is a common type of counting process that models events occurring randomly with a fixed rate. The post explains the properties of the Poisson process and its relationship with exponential and gamma distributions. The post concludes by discussing methods for simulating Poisson processes. The ten most important keywords are: random processes, counting processes, continuous-time random processes, discrete-time random processes, stationary processes, autocorrelation function, cross-correlation function, Poisson process, cyclostationary processes, and gamma distribution.",
    "sha256": "5c0b9dcb35078342c59681a7a8e14698273257787b85fe68249a07cc9d7d1922"
  },
  {
    "id": "/code/2020/05/18/tcc-hpc.html",
    "title": "TCC on HPC",
    "content": "The topological cluster classification (TCC) is a novel tool used to measure the many-body correlations in supercooled liquid and gel systems. This post provides a guide on how to set up TCC on the BlueCrystal supercomputer at the University of Bristol. The general steps include specifying necessary packages in the `.bashrc` file, downloading the source code of TCC from GitHub, compiling TCC using CMake, running TCC directly inside a project directory, and using TCC inside Python. To use TCC on the BlueCrystal, the required packages (GCC, git, and CMake) are already available, but they need to be added to the `.bashrc` file. The modifications can be made using the Vim text editor by appending the necessary lines to the `.bashrc` file. After making the modifications, the `.bashrc` file needs to be sourced to apply the changes. The TCC source code can be downloaded from GitHub using the `git clone` command. To compile TCC, navigate to the TCC directory, create a build directory, run CMake with the appropriate install prefix, and execute the `make` and `make install` commands. Once TCC is compiled, it can be used by navigating to a project directory and running the `tcc` command. The software will then be executed, and the results will be displayed. Additionally, a Python wrapper for TCC is available, and it can be installed by running the `python setup.py install --user` or `pip install . --user` command. The TCC wrapper can then be imported in a Python script, and the TCC executable directory needs to be specified in the code. Finally, a sample Python script is provided to demonstrate the usage of the TCC wrapper. The 10 most important keywords are: topological cluster classification, supercooled liquid, gel systems, setup, BlueCrystal, `.bashrc`, source code, compile, project directory, Python wrapper.",
    "sha256": "175865b28c836ad882363929a094502e2bd5971a01afad90d3c30bd0a115bec9"
  },
  {
    "id": "/code/2019/12/01/html.html",
    "title": "HTML",
    "content": "In this markdown post, the author provides a detailed explanation of various HTML elements and their functionalities. The post starts with an introduction to HTML elements and their syntax. It explains how HTML elements consist of start tags, end tags, and content. The author also mentions that some HTML elements have empty content and can be closed using self-closing tags. The post then discusses the nesting of HTML elements and gives an example of wrapping HTML elements within each other.   Next, the author explains the concept of blank elements that do not have any content. Examples of blank elements are given, such as the `<br>` tag for line breaks. The author also mentions that in XHTML, XML, and future versions of HTML, all elements must be closed. The correct way to close a blank element is by adding a slash inside the start tag.   The post then covers HTML attributes and their usage within HTML elements. The author explains that attributes are defined in the start tags of HTML elements and consist of a name and a value. The value of an attribute should always be enclosed in quotes. Examples of attributes are provided, such as the `href` attribute in the `<a>` tag.   The post also discusses various text formatting elements in HTML, such as `<b>`, `<em>`, and `<small>`, and explains their functionalities. It provides an example of using the `<pre>` tag for displaying pre-formatted text with the same formatting as in the code.   The author then introduces the concept of styling HTML elements using the `style` attribute. It gives an example of styling a `<p>` tag with different text alignment, font-family, and color.   Next, the author explains HTML hypertext, starting with HTML links. The post provides examples of adding links to text and images and discusses opening methods for hyperlinks. It explains how the `target` attribute in the `<a>` tag can be used to specify whether the link should open in the same page or a new browser window. The post also mentions the use of the `title` attribute for displaying prompt text when the cursor is hovered over a hyperlink.   The author then introduces the concept of anchors for intra-page navigation. An example is given of using anchor tags to create jump links within the same page.   The post moves on to HTML tables, explaining their importance in web design and their structure using the `<table>`, `<tr>`, `<td>`, and `<th>` tags. An example of a basic HTML table is provided, along with important tags and attributes for customizing tables, such as `<caption>`, `cellpadding`, `bgcolor`, `rowspan`, and `colspan`.   The next topic covered is HTML images, explaining how to insert images using the `<img>` tag and the `src` attribute. The post provides examples of aligning images and resizing them using the `align`, `width`, and `height` attributes. It also introduces the concept of image maps, where different areas of an image can be linked to different locations.   The post then discusses HTML lists, explaining the syntax for ordered, unordered, and defined lists using the `<ol>`, `<ul>`, `<li>`, `<dt>`, and `<dd>` tags.   The concept of HTML blocks is introduced, explaining the difference between block-level and inline elements. The post provides examples of block-level elements like `<h1>`, `<p>`, `<ul>`, and `<table>`, and inline elements like `<b>`, `<td>`, `<a>`, and `<img>`.   Finally, the post covers HTML layout using tables and divs. It provides examples of creating layouts using the `<table>` and `<div>` elements, along with relevant tags and attributes for customizing the layout.   Keywords: HTML elements, syntax, start tag, end tag, content, empty content, self-closing tags, nesting, blank elements, attributes, text formatting, styling, HTML hypertext, links, opening methods, prompt text, anchors, HTML tables, structure, tags, attributes, HTML images, insertion, align, resize, image maps, HTML lists, ordered list, unordered list, defined list, HTML blocks, block-level elements, inline elements, layouts, tables, divs.",
    "sha256": "8aee7b947e624a5ddcb545f1520707e24029a9e65543fac5f40493e9b1de8279"
  },
  {
    "id": "/code/2019/08/23/css.html",
    "title": "CSS",
    "content": "The provided content is a tutorial on CSS (Cascading Style Sheets). It covers basic and advanced syntax and selectors, including the use of selectors for grouping and inheritance. It also explains different types of selectors, such as ID selectors, class selectors, and attribute selectors, as well as the differences between ID and class selectors. The tutorial further covers basic styling properties for background, text, links, lists, tables, and outlines. It also introduces the concept of the CSS box model, including positioning, floating, and sizing. Lastly, it provides information on advanced topics such as positioning and navigation. The 10 most important keywords are: CSS, selectors, syntax, ID selectors, class selectors, attribute selectors, box model, positioning, floating, and sizing.",
    "sha256": "24c0202c246108d01e346379fabb5eff78e226252a60e05d03f060e32d34da52"
  },
  {
    "id": "/code/2017/04/05/mysql.html",
    "title": "MySQL 简介",
    "content": "This markdown post provides an overview of databases and SQL. It introduces databases as warehouses that organize, store, and manage data using a structured query language called SQL. It also provides an introduction to MySQL, a relational database management system, which stores data in tables to increase speed and flexibility. The post goes on to explain basic commands such as starting the MySQL service, viewing databases, connecting to databases, and exiting. It then covers creating databases and tables, inserting data into tables, and explains the different data types in MySQL. The post also introduces various constraints such as primary keys, default values, unique constraints, foreign key constraints, and non-null constraints. It provides examples of creating and using these constraints. The post then explains the SELECT statement in detail, including basic syntax, using WHERE clauses, sorting results, using built-in functions for calculations, using subqueries, and performing join queries. It also covers modifying and deleting data, such as dropping databases and tables, renaming tables, modifying column properties, and changing column values. Additionally, the post briefly mentions indexing, views, importing data, exporting data, backing up databases, and restoring databases.   The 10 most important keywords are: databases, SQL, MySQL, tables, columns, constraints, SELECT, INSERT, UPDATE, and DELETE.",
    "sha256": "7c979e922e85a9166ab70cdff67a4f80fe8e7f94ab39a4c729b8f6a4b3ca629d"
  },
  {
    "id": "/science/2020/11/22/mathmatics-for-economists.html",
    "title": "Mathematics for Echonomists",
    "content": "This markdown post provides a summary of various mathematical concepts and properties. It starts with an introduction about the authors interest in learning math and mentions a course they are taking. The content is divided into different sections, starting with a section on the properties of operations, such as addition, multiplication, and scalar multiplications. The next section discusses the inner product in vector spaces, including the dot product and the norm of a vector. It also introduces the concept of an open ball and open sets in mathbbR^n. The intersection of open sets and its properties are explained next. Sequences in mathbbR^n and their convergence are discussed, followed by the concept of closed sets and their properties. The operations on closed sets are described, including the intersection and union. The concepts of bounded sets and compact sets are introduced, defining a compact set as a closed and bounded set. The discussion then moves on to functions in mathbbR^n, including their domains and continuity. The Weierstrass theorem is mentioned, stating that a continuous function on a closed interval attains its maximum and minimum values. The notion of composite functions and their continuity are explained, with an example involving the composition of two functions. The post concludes with a diagram illustrating the relationship between the domains and images of composite functions.   The 10 most important keywords are: properties of operations, inner product, open ball, open set, intersection of open sets, sequences, convergence, closed set, bounded set, compact set.",
    "sha256": "f29920c656f174fa8e872abc44d791d874fd35109a51eb23031f50ebd4952a8a"
  },
  {
    "id": "/code/2019/12/17/remove_perspective.html",
    "title": "Removing the Perspective",
    "content": "This post provides a detailed tutorial on how to remove perspective from an image using Python. The post begins by setting up the necessary libraries, including OpenCV, NumPy, PIL, and Matplotlib. It then demonstrates how to load an image using the PIL library. Next, it explains the importance of measuring several points on the image to identify parallel and orthogonal lines. The post includes code to visually represent these measurements on the image. It also explains the relationships between the measured lines. The next step is to calculate the line at infinity, which will be used to recover information about parallel lines in the image. The post provides code to calculate the line at infinity and visualizes it on the image. The homogeneous representation of the line at infinity is also presented. The post then proceeds to create an affine rectified image by applying the transformation obtained from the line at infinity. It provides code and an image showing the result. The next step is to measure coordinates again to ensure that perpendicular lines in real life are also perpendicular in the image. The post includes code and an image to illustrate the measurement results, noting that the perpendicular lines are not yet perpendicular in the image. The post then explains how to calculate the homography from the affine image to a similar image. It provides code and an image to demonstrate this calculation. The resulting similar image is shown, where the chessboard regains its original rectangular shape after the perspective removal. The post emphasizes that the result is not equivalent to an image taken from a top view, as the distances and angles are not invariant in real 3D space. Finally, the post concludes by explaining how to compose the transformation matrix to convert a projective image to a similar image. It provides code and an image showing the result.   The 10 most important keywords are: 1. Perspective 2. Python 3. Libraries 4. Image 5. Measurements 6. Parallel lines 7. Orthogonal lines 8. Affine transformation 9. Homography 10. Similar transformation",
    "sha256": "3141514a36f86e52048d9304bd3f78d29ee718bf637b15bbe8b9743d5f0e15fd"
  },
  {
    "id": "/science/2021/04/04/image-der.html",
    "title": "Modeling Confocal Images",
    "content": "The provided content discusses a model for simulating confocal microscopy images of particles. The model represents each particle as a hard sphere with Gaussian blur in 3D. The contribution of each particle to the image is calculated using the product of its intensity, a Platonic particle without blur, and a Gaussian kernel. The model of the confocal image can be expressed as the sum of these contributions. The content also provides pseudo-code for simulating a confocal image based on a collection of particle locations, radii, and intensity values.   The content then focuses on the derivative of the model and its applications. It introduces the cost function for comparing the model with experimental data and explains that a good model should minimize the squared difference between the model and experimental image. The cost function is a sum over all voxels, where the difference between the experimental image and simulation is weighted by mathrmW. The derivative of the cost function with respect to particle locations is derived, showing that it depends on the derivative of the simulated intensity values and the derivative of the Gaussian blur with respect to the particle locations. The content further simplifies the derivative by considering that the intensity and shape of a particle are not affected by the location of another particle.   The content then presents the numerical implementation of the cost function and its use in updating the particle locations using least square optimization.   In summary, the content discusses a model for simulating confocal microscopy images of particles using a hard sphere with Gaussian blur representation. It explains the calculation of the image contribution for each particle and provides pseudo-code for simulating the image. It also covers the derivative of the model and its application in finding particle locations. Finally, it presents the numerical implementation and use of the cost function.   Keywords: confocal microscopy, particles, hard sphere, Gaussian blur, image simulation, intensity, Platonic particle, Gaussian kernel, cost function, derivative, experimental data, simulated image, numerical implementation, particle locations, least square optimization.",
    "sha256": "7f49824913d93c746ca56e00c39fffc90c1da0be67c83ed38c1e9d7af82e2054"
  },
  {
    "id": "/science/2023/09/08/stat-phy.html",
    "title": "统计物理",
    "content": "The provided content is a Markdown post discussing the Van Hove correlation function and the scaling hypothesis in statistical mechanics. The post begins by introducing the formula for the Van Hove correlation function, which describes the average probability of finding a particle at a certain position at time t given that another particle is at a different position at time 0. The post goes on to explain the relationship between the Van Hove correlation function and the radial distribution function, as well as the distinction between the self and distinct parts of the function. The normalization conditions for the correlation functions are also discussed.   In the second part of the post, the scaling hypothesis is introduced as a way to understand the relationship between magnetization, external magnetic field, temperature, and the critical temperature. The experimental results for magnetization, susceptibility, and scaling functions are presented, followed by the derivation of three relationships using the scaling hypothesis. These relationships involve the magnetization, susceptibility, and scaling functions in different limits. The post concludes by mentioning the predictions made by the scaling hypothesis, including the critical exponents and their relations, which have been experimentally and theoretically verified. The scaling function, although not explicitly described, is mentioned to have a specific shape that can be numerically determined. Overall, the post provides a detailed explanation of the Van Hove correlation function and the scaling hypothesis in statistical mechanics.  Keywords: Van Hove correlation function, density correlation, spatial correlation, temporal correlation, self, distinct, radial distribution function, normalization, scaling hypothesis, magnetization, external magnetic field, temperature, critical temperature, susceptibility, scaling function, critical exponents, relations, experimental verification, theoretical verification.",
    "sha256": "d7c20075f2c788b48638467c2fc5e5c20c005a59a7168b3834e6785e8d92d49a"
  },
  {
    "id": "/science/2020/04/28/vicsek-noise.html",
    "title": "Noise in 3D Vicsek model",
    "content": "This post discusses the implementation of the Vicsek model algorithm in 3D and introduces a method for generating the necessary noise for rotating vectors. The noise is generated by rotating the target direction by a random direction. The post provides math equations to address this process and explains that in the 2D case, the angle of the vector is modified by adding a random angle from a uniform distribution. However, in 3D, things are more complex. The post includes visual graphs to illustrate the effect of the rotation operator and explains the steps to perform this operation. The post then introduces a practical way to generate scalar noise in 3D by generating uniformly distributed noise and rotating it to the direction of the vector. Python code is provided to generate the noise in 3D, and the code is explained in detail. The post also discusses how to pick random points on a sphere and provides the relationship between the noise level and the range of the uniform distribution. Finally, the post explains the rotation process and provides a step-by-step tutorial along with the necessary math equations and illustrations.   Keywords: Vicsek model, noise generation, 3D, rotating vectors, math equations, uniform distribution, scalar noise, Python code, random points on a sphere, rotation process.",
    "sha256": "6518fb104356675a661393916339ade9a301578cbe86845af5369a4f3f87abca"
  },
  {
    "id": "/code/2021/12/17/raw-npy.html",
    "title": "Open NPY in ImageJ",
    "content": "This post provides instructions on how to correctly import NPY files, which are binary representations of Numpy arrays, into ImageJ or FIJI software. The NPY file consists of a header section that contains information about the array, and the raw data section that represents the values in the array. To load the NPY file in ImageJ or FIJI, the user must use the File > Import > Raw option. The user needs to input the correct numbers for width, height, and number of images, taking into account that the order of axes in ImageJ/FIJI is reversed compared to Numpy. Additionally, the user needs to set the Offset to first Image to a value that is divisible by 16, usually 128. The post explains how to determine the length of the header using a hex editor, and provides a code example to dump the array without the header using the `tofile` method in Numpy. Overall, the post offers detailed instructions on how to import NPY files into ImageJ or FIJI, including handling the header length and alternative methods for loading the raw data. The 10 most important keywords are: NPY file, Numpy array, ImageJ, FIJI, header, raw data, File > Import > Raw, axes, Offset to first Image, and tofile method.",
    "sha256": "ddd55b9b5cd6269669ff72aa76d7dafad6aa75db03b388f49c3d5664d5b61e47"
  },
  {
    "id": "/code/2020/03/04/opencv-undistort.html",
    "title": "Opencv Camera Distortion",
    "content": "This post discusses the distortion coefficients used in camera calibration, specifically focusing on how to distort and undistort features in an image without using functions in the OpenCV library. The tutorial provided by OpenCV gives equations for correcting the distortion, but when the equations are implemented, the results are incorrect. An alternative version from a Matlab tutorial shows the correct equations. By examining the source code of OpenCV, it is confirmed that the equations in the OpenCV tutorial are not correct. The post provides a correct implementation of the distortion correction algorithm and shows the correct undistorted results. The keywords for this post are camera calibration, distortion coefficients, undistortion, OpenCV, equations, implementation, source code, correct results, Matlab tutorial.",
    "sha256": "cea620378e3c865cf3b693317212599e6c4d7a479dbc89b2350b84ef57b3aecf"
  },
  {
    "id": "/idea/2022/02/07/read.html",
    "title": "读书笔记",
    "content": "The post discusses the book Meditations by Marcus Aurelius and focuses on a thought-provoking speculation about death mentioned in the book. Marcus Aurelius was a Roman emperor and a philosopher of the Stoic school. In the 10th volume of Meditations, he writes that no one is so fortunate that there wont be someone pleased with their death when they are dying. This idea suggests that even in death, some people may find happiness, unless the deceased person is exceptionally lucky. This dark speculation challenges the commonly held belief that loved ones would mourn our deaths. Marcus Aurelius suggests that we should reflect on this idea and depart from life with contentment, not clinging to it. He emphasizes that we can only change our own thoughts, and it is important to cultivate virtues such as kindness and gentleness without expecting friendship to be a transactional relationship. The post also briefly mentions The School of Life video about Stoicism and lists some common fallacies from the book The Illustrated Book of Bad Arguments. The fallacies include vague generalizations, causal confusion, false dilemmas, circular reasoning, appeal to ignorance, and many others. The detailed explanations and examples of each fallacy are provided.   Keywords: Meditations, Marcus Aurelius, Stoic philosophy, death, speculation, fortunate, dark, thought-provoking, emotions, loved ones, mourning, contentment, virtues, kindness, gentleness, friendship, transactional relationship, The School of Life, Stoicism, fallacies, vague generalizations, causal confusion, false dilemmas, circular reasoning, appeal to ignorance, examples.",
    "sha256": "80624bb25d07d34644f96ce7dab097cf2495bf6a76e06af4ec657026fad64641"
  },
  {
    "id": "/code/2020/06/26/cpp-library.html",
    "title": "Create a C++ Library",
    "content": "This markdown post provides a detailed guide on how to create static and dynamic libraries in C++. The author shares their personal experience of being confused about the C++ library ecosystem and not being able to compile open-source libraries. The post includes example code consisting of three files: `hello_world.h`, `hello_world.cpp`, and `main.cpp`, with the goal of creating a library for `hello_world` that can be used by `main.cpp`. The post then provides instructions for compiling the code with and without the library.  The normal way to compile the code is described, where the `main.cpp` and `hello_world.cpp` files are compiled together using the `g++` command in a Bash script called `compile.sh`. After running the script, the executable `out` is created and executed, resulting in the output Hello World!. The script also includes commands to remove the executable.  The post then explains how to create a static library using the `ar` command. The `hello_world.cpp` file is compiled using the `-c` flag to generate an object file, and then the object file is archived into `libhello_world.a`. The `main.cpp` file is compiled using the `libhello_world.a` library instead of the `hello_world.cpp` file, resulting in the same Hello World! output. The commands to remove the library and the executable are provided.  Next, the post discusses creating a shared library. The `hello_world.cpp` file is again compiled, this time with the `-fPIC` flag to enable sharing, producing an object file named `hello_world.o`. The object file is then used to create a shared library `libhello_world.so` using the `-shared` flag. The `main.cpp` file is compiled with the `-L` flag to specify the library path and the `-l` flag to specify the library name as `hello_world` (ignoring the `lib` prefix and `.so` suffix). The resulting executable, `out`, when executed, prints Hello World!. The commands to remove the object file and the executable are provided.  The keywords for this summary are: C++, eco-system, open source library, static library, dynamic library, compile options, library path configuration, source code, compilation, script, executable, static library archive, object files, shared library, linking, run time, `g++`, `ar`, `-c`, `-fPIC`, `-shared`, `-o`, `-L`, `-l`.",
    "sha256": "8c0a40992d5eb82f40bc773f8424393067ab716b47c09cedc016002618b13b3d"
  },
  {
    "id": "/code/2023/09/09/enum-py-cpp.html",
    "title": "Enumeration in Python/C++",
    "content": "The provided content explains the concept of enumerations (enums) and their usage in Python and C++. Enums are used to represent a set of named values and improve code readability by replacing magic numbers or strings with named constants. The content first introduces the general concept of enums and provides pseudo-code to demonstrate how they can be used. It then focuses on Pythons `enum.Enum`, highlighting its features such as flexible values (enums can be associated with integers, strings, tuples, or other constant values), type-safe comparisons, iterability, and the ability to have methods and attributes. Several examples are provided, including basic enumerations, enums with flexible values, auto value assignment, iterating over enums, and having methods within enums.   The second part of the content discusses C++ enums. It mentions features such as the integral underlying type (enum values are restricted to integral types), scoped enums (better type safety and scope management), and conversions (traditional enums can be implicitly converted to integers, but enum class members require explicit casting). Examples of basic enumerations, scoped enums with specific underlying types, accessing and comparisons, and creating a custom enum using classes are provided.   10 most important keywords: enumerations, enums, named values, code readability, magic numbers, named constants, Python `enum.Enum`, flexible values, type-safe comparisons, C++ `enum`.",
    "sha256": "e6efd97d3d1d7b1e7d12d2ac7dacf6e16951c68fb0ffffb5a8af444645f079cb"
  },
  {
    "id": "/code/2023/09/07/jupyter-notebook.html",
    "title": "Jupyter Notebook Tricks",
    "content": "The provided markdown post covers several topics related to using Jupyter Notebook. It starts by explaining how to reload modules in a Jupyter Notebook using the `%load_ext autoreload` and `%autoreload 2` commands, which ensure that modifications made to a modules code are immediately reflected in the notebooks output. Next, it discusses how to install and uninstall Python kernels for different versions of Python, allowing the user to use them separately and clearly in Jupyter Notebook. The installation process involves using the `ipykernel` package and running `python -m ipykernel install --user --name NAME --display-name NAME`, while uninstallation can be done using `jupyter kernelspec uninstall NAME`. The post then tackles the topic of inserting images into markdown cells. By default, using `!(link_to_image)` inserts an image with a fixed size, but to specify the image size, the user needs to write HTML code, like `<img src=image alt=Drawing style=width: 200px;/>`. However, this rendering method may not work on GitHub. Finally, the post provides instructions on how to connect to the BlueCrystal High-Performance Computing (HPC) system at the University of Bristol using Jupyter Notebook. It involves starting the notebook on BlueCrystal, forwarding the remote port to the local machine, and accessing the notebook through a browser using the provided link. The ten most important keywords are: reload modules, install, uninstall, kernel, Python versions, image, resize, connecting, BlueCrystal, Jupyter Notebook.",
    "sha256": "86dcb3592b4d8304fa93f7b3bd33962b907822a62ae03d426a540968e05a190d"
  },
  {
    "id": "/code/2020/03/02/vim-desktop.html",
    "title": "Vim Desktop in Ubuntu ",
    "content": "The purpose of this markdown post is to explain how to create a desktop file that allows for the use of the latest vim built from the source on a personal working PC. The post provides instructions on creating the desktop file in the `~/.local/share/applications` folder and includes the necessary code to open a file in vim by clicking the file icon. However, simply using the code provided is not enough to make the desktop start up, so the post goes on to explain how to handle this issue. One solution is to modify the `Exec` line in the `.desktop` file to include `bash -c vim %F;exec SHELL`, which prevents the terminal from closing. Another issue to consider is inheriting custom environment variables from the `.bashrc` file. To accomplish this, the `Exec` line can be modified to include `bash -c source ~/.bashrc  vim %F;exec SHELL`. However, caution must be taken with the `.bashrc` file, as certain code blocks need to be placed before any environmental variable declarations in order for the environment to affect the terminal called by the desktop. The post concludes by providing the final version of the `.desktop` file, which includes the necessary modifications and additional details such as the encoding, name, comment, icon, and categories of the application.   Keywords: vim, source, desktop file, file icon, terminal, prevent closing, custom environment, bashrc, variables, code blocks",
    "sha256": "deabaae17a7e98e5ca9a9f9866a3f5241449d4d7eec7d77d37f3cd99fbcfda2d"
  },
  {
    "id": "/code/2017/12/09/struct.html",
    "title": "Python Module Struct",
    "content": "The provided markdown post introduces the `struct` module in Python, which is used to work with binary data. It explains three important methods of this module: `pack`, `calcsize`, and `unpack`. The `pack` method allows us to encode numerical data into formatted binary data. It takes a format string and the values to be packed as arguments and returns a bytes object containing the packed values. An example code snippet is given to demonstrate this functionality. The `calcsize` method is used to determine the size of the struct and the corresponding bytes object produced by `pack`. It takes a format string as an argument and returns the size in bytes. Several example format strings are provided along with their corresponding sizes. Finally, the `unpack` method is used to unpack a buffer of packed data according to a given format string. It takes a format string and the buffer as arguments and returns a tuple of unpacked values. An example code snippet and its corresponding unpacked values are provided to illustrate this functionality. The ten most important keywords in this content are: struct, module, binary data, pack, calcsize, unpack, format string, bytes object, numerical data, and buffer.",
    "sha256": "7796735a1957b289f00aff62c3c67b050122272c67e17f2006a641d607180a89"
  },
  {
    "id": "/code/2021/01/13/connect-pi.html",
    "title": "Connecting Raspeberry Pi",
    "content": "This markdown post describes the process of connecting to a Raspberry Pi from a Ubuntu PC using an ethernet connection. The Raspberry Pi has the Raspberry Pi OS installed and the user spends a significant amount of time figuring out the connection. The post begins by enabling all interfaces on the Raspberry Pi and then using the SSH command `ssh pi@raspberrypi.local` to establish a connection. There are some warnings encountered, so the user runs the command `ssh-keygen -f /home/yushi/.ssh/known_hosts -R raspberrypi.local` to address them. After running the SSH command again, the user is prompted for the default password on the chip, which is raspberry, and the SSH connection is successfully established. The post also mentions the convenience of using VNC Viewer to access the Raspberry Pis desktop. The user installs VNC Viewer on their Ubuntu PC and creates a new connection by entering raspberrypi.local in the VNC Server entry. After providing the username and password, the connection is established.   Keywords: Raspberry Pi, Ubuntu PC, ethernet connection, Raspberry Pi OS, SSH, VNC Viewer, desktop, WIFI networks.",
    "sha256": "2d3bea2fdc939a884b61159649c8f6bc484abdea39c52d7c62ba1253b60bb7e1"
  },
  {
    "id": "/code/2020/02/09/ffmpeg.html",
    "title": "Video Editing",
    "content": "This post provides a collection of useful commands using `ffmpeg` to process videos in the terminal. The author mentions that memorizing these commands is difficult, so they share them here. The commands cover various tasks, such as cutting videos, extracting the first frame, adjusting gamma, removing audio, changing frame rate, generating videos from images, and extracting information about frames and frame rate. The author also includes a command using ImageMagick to create gifs from images. The keywords for this post are ffmpeg, video processing, terminal, command line, cut video, get first frame, change gamma, drop sound, change frame rate, generate video, total frames, frame rate, generate gif, ImageMagick.",
    "sha256": "1e2be2c0eec9c6f4ba55e2d2a01d8f91c79ab1b44bd545dfc3111f51a79e6b75"
  },
  {
    "id": "/code/2023/09/18/chatgpt.html",
    "title": "ChatGPT Prompts",
    "content": "When you ask for an explanation of a command, the ideal response should include a quick overview of the basic syntax, an explanation of the syntax, a simple example, a text description of the command, and a text description of how the command works. For example, if you ask about the `nohup` command, the response would provide the following information:  Command: `nohup COMMAND ARG... ` - `COMMAND`: The command you want to run. - `ARG`: Optional arguments for the command. - ``: Used to run the command in the background.  Example: ```bash nohup python my_script.py  ```  The `nohup` command in Linux allows you to run a process in the background and ensures that it keeps running even after youve logged out. The term nohup stands for No Hang Up. When you run a command with `nohup`, it detaches the process from the terminal and redirects the standard output (`stdout`) and standard error (`stderr`) to a file, usually `nohup.out`.  Keywords: explain, command, syntax, example, description, works, nohup, Linux, process, background.",
    "sha256": "b5dfe8a95ac761cbd14f0a626c7ed7b23667da7648bdbbeba4a1bf8226c5be37"
  },
  {
    "id": "/science/2019/05/17/arguments.html",
    "title": "论证笔记",
    "content": "The content is about the nature and structure of arguments. The author defines an argument as a set of sentences in which one sentence is being asserted as true and the others are being offered as reasons for believing the truth of the asserted sentence. The author also provides an example to illustrate the structure of an argument, with a conclusion and premises. It is pointed out that arguments have different functions for the conclusion and premises, and it is the only way to determine which category a sentence belongs to. The content also discusses sets of sentences that are not arguments, explaining that they either have no relation between them or have a relation other than that characterizing an argument. The author emphasizes that the relationship between sentences is crucial in determining whether they form an argument. The difference between an argument and an assertion is also explained, with an argument being a set of sentences, one of which is being asserted, while an assertion is a single sentence being expressed in assertoric mode. The author also explores the concepts of truth and falsity, stating that only beliefs or sentences expressing beliefs can be true or false, and arguing that arguments themselves cannot be true or false but can only be good or bad. The content also introduces three separate levels - language, thought, and reality - and explains their relationship in discussing facts. The author concludes this part with the characteristics of a good argument, stating that the conclusion must follow from the premises and the premises must all be true. The second part of the content discusses different types of arguments: deductive and inductive. Deductive arguments are defined as arguments in which the truth of the premises guarantees the truth of the conclusion, while inductive arguments are arguments in which the truth of the premises makes the conclusion more or less probable. The author provides examples to illustrate both types of arguments and highlights the importance of the assumption of the uniformity of nature in inductive arguments. The content also explores the forms of deduction and induction in more detail and discusses different forms of deduction, including analogous arguments, cause arguments, moral arguments, modal logic arguments, and temporal logic arguments. The author concludes by discussing the steps for analyzing arguments, including identifying the conclusion, premises, suppressed premises, removing irrelevancies and inconsistent terms, and cross-references. The 10 most important keywords are nature of arguments, definition of argument, structure of an argument, premises, conclusion, deductive argument, inductive argument, form of deduction, form of induction, and analyzing arguments.",
    "sha256": "b0cc635c3fd8f5462b2308cd1983882366e56c7cf6018347817d5ab02fb96878"
  },
  {
    "id": "/code/2020/11/15/ml.html",
    "title": "机器学习",
    "content": "The provided content is a markdown post that explains the usage of the Keras Functional API in Tensorflow 2. The post starts by introducing the Keras module in Tensorflow 2, which provides an application programming interface (API) for building neural network models in a functional style. The post then compares the object-oriented API with the functional API in creating models. In the object-oriented API section, an example code is shown using the Sequential function to create a model with three layers. In the functional API section, the same model is built using a functional style, where objects are created using the tf.keras.Input and tf.keras.layers.Dense functions, and the model is created using the tf.keras.Model function. The post goes on to provide a deeper understanding of the functional API, explaining that the functions in tf.keras.layers actually return another function. An example code is given to demonstrate this behavior. Finally, the post discusses the scenario of creating a model without specifying the input shape and mentions that such a model cannot be summarized until it has been used once. An example code is provided to illustrate this, where the model is first used with an input of shape (1, 5) and then attempted to be used with an input of shape (1, 8), resulting in an error. The post concludes with a reference to the Tensorflow documentation and a message saying that the model cannot be applied to inputs of different shapes. The 10 most important keywords in this post are: Keras, API, Tensorflow, object-oriented, functional style, layers, Sequential, Input, Dense, model.",
    "sha256": "778ba0a4e0210c4c3d81b576f20ad1ee4ffcbe4b3f37bf1a798f2ae3071759cc"
  },
  {
    "id": "/science/2020/11/02/pbc_py.html",
    "title": "Pairwise distance with PBC",
    "content": "This markdown post discusses different implementations of the periodic boundary condition (PBC) in Python simulations. The PBC is commonly used to simulate systems with molecular dynamics or Monte Carlo sampling and ensures that the volume fraction of the system remains fixed. The post introduces three different approaches for implementing the PBC.   The first approach involves calculating pair distances using nested for loops. Although this method is straightforward, it can be slow due to the inherently slow nature of nested for loops in Python. The second approach utilizes the `pdist` function from the `scipy` package to calculate pair distances. This method is significantly faster than the first approach. The third approach is based on an implementation from Allen  Tildesleys book and involves calculating pairwise shifts and distances. Although this method is less efficient than the second approach, it provides access to the pairwise shift, which can be useful for certain agent-based models. The post provides code examples for each approach and compares their efficiency.  The keywords in this summary are: molecular dynamics, Monte Carlo sampling, periodic boundary condition, PBC, pair distances, Lennard-Jones potential, pairwise potentials, nested for loop, pdist, scipy, Allen  Tildesley, efficiency, Python code.",
    "sha256": "479cd90821e346e2ae3124b8aa9171a1b172bab5aedfae8f9cf6f19b0b805757"
  },
  {
    "id": "/science/2022/01/18/math.html",
    "title": "数学",
    "content": "The post discusses four topics: functional derivatives, Markov chains, simplex, and matrix norms.   Functional derivatives are used to understand how small changes in the input function affect the output value of a functional mapping from functions to real numbers. It is analogous to the derivative in calculus and is denoted as δFf/δf(y). The process of computing functional derivatives involves introducing a variation in the function, expressing the functional in terms of the variation, Taylor expanding around the variation, and extracting the coefficient of the linear term.  Markov chains are stochastic processes where the future state depends only on the current state, not on the past states. The transition probabilities between states are represented by a transition matrix. The properties of a Markov chain are described in terms of accessibility, communication, recurrence, and transience. The hitting time matrix and the transition matrix describe the long-term behavior of the Markov chain.  The simplex is a geometric concept that represents a set of points in multi-dimensional space. In two dimensions, it is a line segment, in three dimensions, it is a triangle, and in higher dimensions, it is an n-dimensional object. The simplex is closely related to probability distributions, as a point in the simplex represents a probability distribution over a set of outcomes.  Matrix norms, specifically the nuclear norm and spectral norm, are measures of the size or effect of a matrix. The nuclear norm is the sum of the singular values of a matrix and can be used for matrix approximation and low-rank matrix recovery. The spectral norm is the maximum singular value of a matrix and is used for matrix approximation with a spectral norm constraint. Both norms can be integrated into semidefinite programming optimization problems.  Keywords: functional derivatives, Markov chains, simplex, matrix norms, transition matrix, hitting time matrix, accessibility, communication, recurrence, transience, nuclear norm, spectral norm, semidefinite programming, optimization",
    "sha256": "12be445844df336241d10f9ee7d49ea6ea456139d5385261f5eb65dca28d46f2"
  },
  {
    "id": "/idea/2023/10/07/heart-sutra.html",
    "title": "Heart Sūtra",
    "content": "The Heart Sutra is a Mahayana Buddhist text that explores the concept of emptiness (空) and aims to help practitioners comprehend the nature of reality and achieve enlightenment. It challenges conventional understandings of reality and existence, emphasizing key phrases like Form is emptiness; emptiness is form and No Ignorance, and also no end of ignorance. The sutra begins with Avalokiteshvara Bodhisattva practicing ultimate wisdom, realizing that the five aggregates of human existence are empty. This wisdom leads to liberation from suffering. The text further explains that all phenomena are inherently empty and lack independent existence, including feelings, perceptions, impulses, consciousness, sensory organs, sensory objects, realms of operation, abstract concepts, and even the Four Noble Truths. Understanding emptiness frees the mind from obstacles and fear, ultimately leading to ultimate Nirvana. The concluding lines highlight the importance of relying on Prajna Paramita, the wisdom of emptiness, to attain enlightenment. The Wisdom of Prajna Paramita is praised as a powerful mantra capable of removing all suffering, and it is proclaimed in the mantra Gate gate paragate parasamgate bodhi svaha which encapsulates the teachings of the Heart Sutra. The Sutra also compares Platos Forms with Buddhist Emptiness, noting similarities in the impermanence of the material world and the concept of immutability, but highlighting differences in the source of imperfection, ultimate reality, engagement with the world, and ontological status. Overall, the Heart Sutra emphasizes the understanding of emptiness to transcend suffering and attain enlightenment.  Keywords: Heart Sutra, Mahayana Buddhist, emptiness, reality, enlightenment, Avalokiteshvara Bodhisattva, Prajna Paramita, five aggregates, liberation, form, ignorance, phenomena, sensory organs, sensory objects, realms, Four Noble Truths, Nirvana, mantra, Prajna Paramita mantra, suffering, Platos Forms, Buddhist Emptiness, impermanence, immutability, source of imperfection, ultimate reality, engagement with the world, ontological status.",
    "sha256": "1f7164f377bb239b601948f442178a99ae37ffae7adac704af0ffb1fa78cfc23"
  },
  {
    "id": "/code/2023/10/24/libgomp-issue.html",
    "title": "incident about invalid libgomp",
    "content": "This markdown post discusses a strange issue encountered when compiling R from source code. The problem was related to the `libgomp.so` library and its dependencies. The author was working on a remote server without sudo privilege, so everything was compiled from source code and placed under the `HOME/.local` directory. The author had recently compiled their own gcc-11 with the gcc-7.5 provided by the server, but encountered an error when trying to compile R with the new gcc-11. The root cause of the issue was identified as a symbolic link (`libgomp.so`) pointing to a different library (`libomp.so`). This unconventional setup caused linker errors, as the R compilation process expected the genuine `libgomp.so` library. The misconfiguration could have been caused by manual intervention, a mix-up during package management or installation, or a misconfiguration during the custom build of GCC or another software. To resolve the issue, the problematic symlink was renamed, and after that, the R source code compilation proceeded without errors. The post also mentions future precautions to take, such as documenting system configuration expedients, using version control, reviewing and auditing custom configurations periodically, and employing isolated environments for experimental work or testing. It is also advised to regularly back up systems, especially before making major changes. The post mentions some diagnostic tools (`ldd`, `file`, `objdump`, and `readelf`) that were used to identify the problem by checking shared library dependencies and displaying information about object files and ELF binaries. The issue highlights the importance of correctly setting up system configurations, especially library linkages, and the use of diagnostic tools for troubleshooting. It emphasizes the need for careful documentation and periodic review of system modifications.   Keywords: compiling, R, source code, libraries, libgomp.so, remote server, sudo privilege, gcc-11, gcc-7.5, linker errors, misconfiguration, symbolic link, libomp.so, manual intervention, package management, installation, custom build, resolution, future precautions, documentation, version control, review, audit, isolated environments, backup, diagnostic tools, ldd, file, objdump, readelf, system configurations, library linkages, troubleshooting.",
    "sha256": "dde617e9f321ea8098b1245853b18b07571c594522ca8ea02227178ac2d4a10a"
  },
  {
    "id": "/work/2023/08/18/ngs.html",
    "title": "二代测序",
    "content": "In the provided markdown post, the content discusses the basics of DNA sequencing and the concept of barcodes in sequencing. It explains that DNA is composed of two complementary strands with a 5 and 3 end, and the standard DNA sequence direction is from 5 to 3. Pair Ended (PE) sequencing is introduced as a method to obtain sequence information from both ends of a DNA fragment, providing directionality and distance information. Examples are given for PE sequencing, showing the original DNA fragment sequence and the resulting read sequences.   The post then delves into the concept of barcodes in sequencing. Barcodes are unique DNA sequences added to each fragment in order to differentiate samples in a sequencing experiment. The different types of barcodes are discussed, including single-ended barcodes added to one end of the fragment, double-ended barcodes added to both ends of the fragment, and single-read barcodes added to only one specific read in paired-end sequencing. Examples are provided for each type of barcode, showing the modified fragment sequences and resulting read sequences.   The concept of strand bias is briefly mentioned, defining it as the uneven representation of DNA strands in sequencing data.   The post then transitions to a discussion on probability modeling using Markov Chains. It explains that Markov Chains can be used to calculate the probability of observing a specific class of bases (ACGT) after a certain number of observed bases. The transition matrix for the Markov Chain is shown, along with the calculation of the final state probabilities using matrix multiplication. The post provides code examples in Python to calculate the final state probabilities using exact calculation and Monte Carlo simulation, and visualizes the results using a graph.  Overall, the content covers sequencing basics, barcoding in sequencing, strand bias, and probability modeling using Markov Chains. The 10 most important keywords are: DNA sequencing, barcodes, PE sequencing, DNA fragments, directional information, distance information, single-ended barcodes, double-ended barcodes, single-read barcodes, Markov Chains.The provided content is a Markdown post that includes an SVG image and a code block. The SVG image contains a graph with multiple lines and patches, which represent various data points. The graph also includes a legend that provides information about the different lines on the graph. The code block includes two timings: one for the exact calculation and one for the Monte Carlo simulation. The exact calculation took 0.07 ms, while the Monte Carlo simulation took 1170.33 ms. The post also includes a note that the speed of the exact calculation is 16714 times faster than the Monte Carlo simulation^1^2.   Keywords: Markdown, SVG, image, graph, lines, patches, legend, code block, timing, exact calculation, Monte Carlo simulation.",
    "sha256": "f6e78e01e67634e51f41bae1d389461d7784f1283fa0be7413b57927935fca20"
  },
  {
    "id": "/code/2023/10/26/mem-align-cpp.html",
    "title": "Memory Alignment in C++",
    "content": "Memory alignment refers to how data is organized and accessed in computer memory. It is important for efficient access and proper functioning of a given architecture. Aligned memory access, or accessing memory on its alignment boundary, can lead to faster operations. On the other hand, misaligned memory access can result in performance penalties or even crashes, particularly with certain processors and SIMD instruction sets. The C++11 standard introduced the `alignof` operator, which allows querying the alignment requirement of a specific type. Basic data types often have alignment requirements that match their size, such as 4-byte alignment for an `int` and 8-byte alignment for a `double`. Vector types in C++ behave like any other data type and can be passed as parameters, returned from functions, and used in arrays. However, pointers to vector data need to point to aligned memory blocks, usually with a multiple of 32-byte alignment. Failing to meet the alignment requirement can result in program crashes. To ensure alignment, newer compilers can use standard containers with vector types (e.g., `std::vector<float8_t>`) while older compilers can use functions like `posix_memalign` or `aligned_alloc`.   Classes and structs have alignment requirements that derive from their members. The alignment requirement of a class or struct is the maximum alignment requirement of its members, and the size is usually a multiple of this requirement. To ensure proper alignment, compilers may insert padding between members. Padding can be controlled or eliminated using compiler directives like `#pragma pack`. Reordering members based on alignment can also reduce padding, optimizing memory usage. Another technique for compact storage is using bit-fields, which allows specifying a fixed number of bits for an integer type within a structure. Bit-fields are commonly used in scenarios like protocol headers, hardware register representations, and compact data structures. SIMD architectures, such as SSE and AVX on x86 processors, have specific alignment requirements. For example, if a vector type uses AVX instructions and handles 8 single-precision floating-point numbers (occupying 256 bits or 32 bytes), it would require 32-byte alignment. Failing to meet the alignment requirement in such cases can result in crashes. It is crucial to correctly allocate and free memory when using methods like `posix_memalign` or `aligned_alloc` to avoid errors.   The 10 most important keywords are: memory alignment, data types, aligned memory access, misaligned memory access, C++11, alignof, basic data types, vector types, classes and structs, bit-fields.",
    "sha256": "482636c8437b89a06de99c77c68a71f5a8a69be5591dc36f26e41adbef544ba4"
  },
  {
    "id": "/code/2023/10/16/compile-gcc.html",
    "title": "Compile GCC offline",
    "content": "In this markdown post, the author describes the general steps for building a custom version of `gcc`, which is often necessary in order to utilize new features in C++. The process involves downloading the source code and performing an out-of-source build. Additionally, there are dependencies that need to be installed, including libmpc, mpfr, gmp, and isl. After configuring the build, the code is then compiled using the make command. The author also provides a note that mentions the need to unset C/C++ related flags using the unset command.   Keywords: gcc, C++, source code, out-of-source build, dependencies, libmpc, mpfr, gmp, isl, configure, compile, make, unset.",
    "sha256": "35b188d8f0c58b4015294865d8022352c82f7dea1c0c27a93c2de82cc25bc01b"
  },
  {
    "id": "/code/2021/04/14/cpp.html",
    "title": "C++",
    "content": "The provided content is a markdown post that discusses several concepts related to C++ programming, including iterations, concurrency, memory management, and object-oriented programming.  In the section on iterations, the post explains different looping techniques, such as traditional for loops, while and do-while loops, pointer-based iterations, and iterations using the Standard Template Library (STL) with iterators and range-based loops. The post also introduces algorithms that operate on iterator ranges, as well as parallel iterations introduced in C++17.  The section on concurrency discusses different programming models for managing multiple tasks that may overlap or interleave. It explains the lack of built-in concurrency support in pre-C++11 standard libraries and the reliance on platform-specific libraries or Boost.Thread. Then it highlights the concurrency features introduced in C++11, including memory model, threads, mutexes, condition variables, atomic operations, and futures. It also mentions the parallel algorithms and concurrency features introduced in C++17 and later versions, such as parallel algorithms and coroutines.  The section on memory management covers dynamic memory allocation using `new` and `delete`, the concept of pointers, and the usage of smart pointers to handle resource management. It explains different types of smart pointers, such as `std::unique_ptr`, `std::shared_ptr`, and `std::weak_ptr`, and their respective use cases. It also discusses the concept of ownership and the RAII (Resource Acquisition Is Initialization) principle.  The section on object-oriented programming discusses concepts such as structures and classes, access modifiers (public, protected, and private), operator overloading, static members, and virtual functions. It explains the differences between structures and classes and the different access levels of members. It also discusses the usage of virtual functions for achieving polymorphism and runtime dispatching. It highlights the use of virtual functions for interface abstraction, dynamic dispatching, the template method pattern, and resource management.  Finally, the post briefly mentions some optimization techniques, such as inline functions, linear access, and avoiding non-deterministic behavior.  Keywords: C++, iterations, concurrency, memory management, smart pointers, object-oriented programming, virtual functions, optimization.",
    "sha256": "42c693bbd507d79cb2f7c8e8a283fb73af250a11396b7defda66d1745302250b"
  },
  {
    "id": "/code/2020/10/22/hpc.html",
    "title": "Useful Linux Commands",
    "content": "This markdown post provides a collection of useful command line scripts and commands for various tasks. The `find` command is used to search for files and directories based on different criteria such as name, type, size, permissions, and modification date. The `rsync` command is used for synchronizing files and directories between two locations, leveraging a delta-transfer algorithm to minimize data transfer. The `nohup` command allows running a process in the background and keeps it running even after logging out. The `pbcopy` and `xclip` commands are used to copy input to the clipboard on macOS and Linux systems respectively. Additionally, the post provides commands for printing file size, generating hash values, checking the OS distribution, converting image formats, displaying images, and compiling software such as Vim and Python. It also includes commands specific to HPC tasks, such as transferring files to and from a remote server and specifying memory requirements. The keywords are `find`, `rsync`, `nohup`, `pbcopy`, `xclip`, file size, hash value, OS distribution, image, compile, HPC, transfer files, memory.",
    "sha256": "6fc671bdc353500c353e8a6e9b0f9e600fa7c617e6e873b3db1b1f9da16feec1"
  },
  {
    "id": "/code/2023/11/19/onnx.html",
    "title": "Neural Network represented by ONNX",
    "content": "A machine learning model can be represented as a computation graph in the ONNX format. The graph consists of nodes and edges, with nodes representing input/output tensors or operations. Additionally, the graph includes initializers, which are used as constants within the graph, and attributes, which are fixed parameters of an operator. The `onnxruntime` library provides APIs in C++, Python, and C# to execute ONNX models on various platforms like CPU and GPU. It also offers model optimization techniques such as operation fusion and quantization. `onnxruntime` supports different execution providers (EP), allowing the same codebase to run on different devices by changing the EP configuration. The standard release version includes 3 EPs: `CPUExecutionProvider`, `CUDAExecutionProvider`, and `TensorrtExecutionProvider`. To use additional EPs, the `onnxruntime` needs to be built from source code. Profiling the model is essential for identifying the slow parts of the execution process. This can be done by enabling profiling in the `SessionOptions` instance of `onnxruntime` and generating a JSON file with the start and end times of each node. The JSON file can be visualized using the Chrome tracing tool. Other tools mentioned include Netron, which is a viewer for neural network models, and ONNX GraphSurgeon, a Python package for editing ONNX computation graphs. Keywords: machine learning model, computation graph, ONNX representation, nodes, edges, initializers, attributes, `onnxruntime`, APIs, execution providers, model optimization, operation fusion, quantization, `CPUExecutionProvider`, `CUDAExecutionProvider`, `TensorrtExecutionProvider`, profiling, `SessionOptions`, JSON file, Chrome tracing tool, Netron, ONNX GraphSurgeon.",
    "sha256": "379472e3b52fe7cec45ee159a3554fb70dda0f80a4abebb76c16a7cfda013295"
  }
]