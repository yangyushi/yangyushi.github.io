<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Sequence Model in TensorFlow</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="stylesheet" href="/assets/css/styles/github-gist.css">
    <script src="/assets/js/toc.js"></script>
    <script src="/assets/js/highlight.pack.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>

    <script type="text/javascript" id="MathJax-script" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
      >
    </script>
    
    </head>

  <body>
    <script>hljs.initHighlightingOnLoad();</script>
    
    <nav>
    <ul>
        
        <li>
        <a href=/
            
        >
            About
        </a>
        </li>
        
        <li>
        <a href=/notebook.html
            
        >
            Notebook
        </a>
        </li>
        
    </ul>
</nav>


    <div class="main">
        <div id="side_bar">
    03 Apr 2022
</div>

<div class="post center">
    <ul id="markdown-toc">
  <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a>    <ul>
      <li><a href="#definition" id="markdown-toc-definition">Definition</a></li>
      <li><a href="#what-to-do" id="markdown-toc-what-to-do">What to Do</a></li>
      <li><a href="#typical-patterns" id="markdown-toc-typical-patterns">Typical Patterns</a>        <ul>
          <li><a href="#stationary" id="markdown-toc-stationary">Stationary</a></li>
          <li><a href="#non-stationary" id="markdown-toc-non-stationary">Non-stationary</a></li>
        </ul>
      </li>
      <li><a href="#how-to-predict" id="markdown-toc-how-to-predict">How to Predict</a></li>
      <li><a href="#how-to-evaluate-prediction" id="markdown-toc-how-to-evaluate-prediction">How to Evaluate Prediction</a></li>
      <li><a href="#predicting-without-ml" id="markdown-toc-predicting-without-ml">Predicting without ML</a></li>
    </ul>
  </li>
  <li><a href="#prediction-with-dnn" id="markdown-toc-prediction-with-dnn">Prediction with DNN</a>    <ul>
      <li><a href="#preprocessing" id="markdown-toc-preprocessing">Preprocessing</a>        <ul>
          <li><a href="#introducing-to-dataset" id="markdown-toc-introducing-to-dataset">Introducing to Dataset</a></li>
          <li><a href="#generating-training-data" id="markdown-toc-generating-training-data">Generating Training Data</a></li>
        </ul>
      </li>
      <li><a href="#linear-regression" id="markdown-toc-linear-regression">Linear Regression</a></li>
      <li><a href="#simple-dnn" id="markdown-toc-simple-dnn">Simple DNN</a></li>
    </ul>
  </li>
  <li><a href="#prediction-with-rnn" id="markdown-toc-prediction-with-rnn">Prediction with RNN</a>    <ul>
      <li><a href="#overview" id="markdown-toc-overview">Overview</a></li>
      <li><a href="#the-model" id="markdown-toc-the-model">The model</a></li>
      <li><a href="#using-lstm" id="markdown-toc-using-lstm">Using LSTM</a></li>
      <li><a href="#using-convolution" id="markdown-toc-using-convolution">Using Convolution</a></li>
    </ul>
  </li>
</ul>
<hr />

<p>This is the note I took when learning <a href="https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/home/welcome">this course</a>.</p>

<hr />

<h1 id="introduction">Introduction</h1>

<h2 id="definition">Definition</h2>

<p>Times seires is defined as</p>

<blockquote>
  <p>an ordered sequence of values that are usually <em>equally spaced</em> over time</p>
</blockquote>

<h2 id="what-to-do">What to Do</h2>

<p>We can use ML for the follwoing applications</p>

<ol>
  <li>Predicting the future (forecasting)</li>
  <li>Retracing the past (imputation &amp; Interpolate)</li>
  <li>Detection of the anomaly (spikes in time series)</li>
  <li>Detection of patterns</li>
</ol>

<h2 id="typical-patterns">Typical Patterns</h2>

<h3 id="stationary">Stationary</h3>

<p>Time series are ideally from a stationary stochastic process, with following patterns.</p>

<ol>
  <li>Trend (linear).</li>
  <li>Sensonality (periodicity).</li>
  <li>White noise (not learnable).</li>
  <li>Auto-correlation (often shown as <em>deterministic decay</em>)</li>
</ol>

<p>These four basic types are often combined for real data.</p>

<h3 id="non-stationary">Non-stationary</h3>

<p>In real life, the data may not be a <em>stationary random process</em>, so everything can change!</p>

<p>These dataset are called <em>none stationary time series</em>.</p>

<p>We chop a small and stable section of the entire time series, to make some predictions.</p>

<p>(more data may not be better!)</p>

<p>We just can not predict drastic changes!</p>

<h2 id="how-to-predict">How to Predict</h2>

<p>We split the time series into different sections, to do the ML.</p>

<ol>
  <li>Training period</li>
  <li>Validation period</li>
  <li>Test period</li>
</ol>

<p>There are different ways to segment the time series.</p>

<ul>
  <li>fixed partitioning: <code class="language-plaintext highlighter-rouge">[train]- [validation]- [test]</code> 
(If there is seasonality, each partition should have multiple seasons).</li>
  <li>roll-forward partition: gradually increase the <code class="language-plaintext highlighter-rouge">[train]</code> size, but fix <code class="language-plaintext highlighter-rouge">[validation]</code> and <code class="language-plaintext highlighter-rouge">[test]</code>
(it is like doing fixed partitioning multiple time to refine gradually)</li>
</ul>

<h2 id="how-to-evaluate-prediction">How to Evaluate Prediction</h2>

<p>There are following matrices
\(\begin{aligned}
\mathrm{mse} &amp;= \frac1N \sum_i (\hat{y}_i - y_i)^2 \\
\mathrm{rmse} &amp;= \sqrt{\mathrm{mse}} \\
\mathrm{mae} &amp;= \frac1N \sum_i \vert \hat{y}_i - y_i \vert \\
\mathrm{mape} &amp;= \frac1N \sum_i \vert \frac{\hat{y}_i - y_i}{y_i} \vert  &amp; \text{p = percentage}
\end{aligned}\)</p>

<ul>
  <li>$\hat{y}_i$ is the predicted value at time $i$</li>
  <li>$y_i$ is the true value at time point $i$</li>
</ul>

<h2 id="predicting-without-ml">Predicting without ML</h2>

<p>There are ways to predict time series with out ML, called the <em>statistical forcasting</em>.</p>

<ul>
  <li>Naive forcasting: $\hat{y}_{i+1} = y_i$ (the result in that course is just wrong.)</li>
  <li>Moving average: $\hat{y}<em>i = \frac1S\sum</em>{j=0}^{j=S} \hat{y}_{i-j} $</li>
  <li>Differencing: try to predict the difference of the time series, rather than the time series.
(Differencing removes the <em>trend</em> and the <em>seasonality</em>)</li>
</ul>

<h1 id="prediction-with-dnn">Prediction with DNN</h1>

<h2 id="preprocessing">Preprocessing</h2>

<h3 id="introducing-to-dataset">Introducing to Dataset</h3>

<p>Tensorflow offered a nice <code class="language-plaintext highlighter-rouge">Dataset</code> class, to obtain data. Here are some examples.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">val</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="sh">"""</span><span class="s"> result
0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>We can use the <code class="language-plaintext highlighter-rouge">window</code> method to generate nested (2D) dataset from 1D arrays.</p>

<p>Notice, the results can not be converted to <code class="language-plaintext highlighter-rouge">numpy</code> directly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="c1"># data.numpy() -&gt; _VariantDataset' object has no attribute 'numpy'
</span>    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">val</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">()</span>

<span class="sh">"""</span><span class="s"> result
0, 1, 2, 3, 4, 
1, 2, 3, 4, 5, 
2, 3, 4, 5, 6, 
3, 4, 5, 6, 7, 
4, 5, 6, 7, 8, 
5, 6, 7, 8, 9, 
6, 7, 8, 9, 
7, 8, 9, 
8, 9, 
9, 
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>We can use <code class="language-plaintext highlighter-rouge">drop_remainder</code> to remove the result, whose length is smaller than widnow size.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">val</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">()</span>

<span class="sh">"""</span><span class="s">result
0, 1, 2, 3, 4, 
1, 2, 3, 4, 5, 
2, 3, 4, 5, 6, 
3, 4, 5, 6, 7, 
4, 5, 6, 7, 8, 
5, 6, 7, 8, 9, 
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>We can also transofrm a block of array into <code class="language-plaintext highlighter-rouge">numpy</code> directly, using the <code class="language-plaintext highlighter-rouge">flat_map</code> method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    
<span class="sh">"""</span><span class="s">result
[0 1 2 3 4]
[1 2 3 4 5]
[2 3 4 5 6]
[3 4 5 6 7]
[4 5 6 7 8]
[5 6 7 8 9]
</span><span class="sh">"""</span>
</code></pre></div></div>

<h3 id="generating-training-data">Generating Training Data</h3>

<p>For the sake of predicting time series, we split the data <code class="language-plaintext highlighter-rouge">1, 2, 3, 4, 5</code> into</p>

<ul>
  <li>The features <code class="language-plaintext highlighter-rouge">1, 2, 3, 4</code> (the prediction will base on these numbers)</li>
  <li>The label <code class="language-plaintext highlighter-rouge">5,</code> (the prediction will predict this value)</li>
</ul>

<p>This can be done with the <code class="language-plaintext highlighter-rouge">map</code> method</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]))</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    
<span class="sh">"""</span><span class="s">result
[0 1 2 3] [4]
[1 2 3 4] [5]
[2 3 4 5] [6]
[3 4 5 6] [7]
[4 5 6 7] [8]
[5 6 7 8] [9]
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>We can also <code class="language-plaintext highlighter-rouge">shuffle</code> the data, to avoid the <a href="https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/supplement/Qo0TU/sequence-bias"><em>sequence bias</em></a>, which is defined as</p>

<blockquote>
  <p>Sequence bias is when the order of things can impact the selection of things.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    
<span class="sh">"""</span><span class="s">result
[0 1 2 3] [4]
[5 6 7 8] [9]
[2 3 4 5] [6]
[1 2 3 4] [5]
[4 5 6 7] [8]
[3 4 5 6] [7]
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>Finally, we separate the dataset into different <em>batches</em>, for the training of the network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="n">shape</span><span class="p">)</span>
    
<span class="sh">"""</span><span class="s">result
(2, 4) (2, 1) -&gt; x = [[1, 2, 3, 4], [3, 4, 5, 6]], y = [[5,], [6,]]
(2, 4) (2, 1)
(2, 4) (2, 1)
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>To generate dataset from an existing time series, we can use the following function, which includes all previous methods.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">windowed_dataset</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle_buffer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Args:
        series (np.ndarray): 1D array
        window_size (int): the length + 1 of the features
        batch_size (int): the number of instances in a batch for training
        suffle_buffer (int): the buffer for randomnise dastaset
    </span><span class="sh">"""</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">(</span><span class="n">series</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">window</span><span class="p">:</span> <span class="n">window</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">shuffle_buffer</span><span class="p">).</span><span class="nf">map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">window</span><span class="p">:</span> <span class="p">(</span><span class="n">window</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">window</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>
</code></pre></div></div>

<h2 id="linear-regression">Linear Regression</h2>

<p>We can use linear regression to fit the data, by using a NN with one layer. Mathematically, the calculation is written as</p>

\[y_i = \left( \begin{matrix}
w_1 \\ w_2 \\ \vdots \\ w_n
\end{matrix} \right)
\left( \begin{matrix} 
y_{i+(0-n)} &amp; y_{i+(1-n)} &amp; \dots &amp; y_{i+(n-1-n)}
\end{matrix} \right) + b.\]

<p>This approach will not be very successful. It’s result is similar to taking the window average.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">window_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">shuffle_buffer_size</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nf">windowed_dataset</span><span class="p">(</span>
  <span class="n">x_train</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle_buffer_size</span>
<span class="p">)</span>

<span class="n">l0</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="n">window_size</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span><span class="n">l0</span><span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">mse</span><span class="sh">"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span>
  <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="simple-dnn">Simple DNN</h2>

<p>We can use a deeper network to fit the data, with minimum modification. This yields slightly better result.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">window_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">shuffle_buffer_size</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="nf">windowed_dataset</span><span class="p">(</span>
  <span class="n">x_train</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle_buffer_size</span>
<span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="n">window_size</span><span class="p">]),</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">),</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">mse</span><span class="sh">"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span>
  <span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>
<span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span><span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>We can schedule the <em>learning rate</em> while training, with the <code class="language-plaintext highlighter-rouge">LearningRateScheduler</code> object.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">lr_schedule</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">callbacks</span><span class="p">.</span><span class="nc">LearningRateScheduler</span><span class="p">(</span>
    <span class="k">lambda</span> <span class="n">epoch</span><span class="p">:</span> <span class="mf">1e-8</span> <span class="o">*</span> <span class="mi">10</span><span class="o">**</span><span class="p">(</span><span class="n">epoch</span> <span class="o">/</span> <span class="mi">20</span><span class="p">))</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">optimizers</span><span class="p">.</span><span class="nc">SGD</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">)</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
  <span class="n">loss</span><span class="o">=</span><span class="sh">"</span><span class="s">mse</span><span class="sh">"</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="n">optimizer</span>  <span class="c1"># the SGD object allows adjusting LR
</span><span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span>
  <span class="n">dataset</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
  <span class="n">callbacks</span><span class="o">=</span><span class="p">[</span><span class="n">lr_schedule</span><span class="p">]</span>
<span class="p">)</span>
</code></pre></div></div>

<h1 id="prediction-with-rnn">Prediction with RNN</h1>

<h2 id="overview">Overview</h2>

<p>Here is the structure of our RNN.</p>

<pre class="asciiart">
                     output (forecasts)
               shape: (batch_size, time_steps, )
                            ▲         
                            │         
                  ┌──────────────────┐
                  │   Dense Layer    │
                  └──────────────────┘
                            ▲         
                            │         
                  ┌──────────────────┐
                  │ Recurrent Layer  │
                  └──────────────────┘
                            ▲         
                            │         
                  ┌──────────────────┐
                  │ Recurrent Layer  │
                  └──────────────────┘
                            ▲         
                            │         
                         input x
      shape (batch_size, time_steps, dimensions)
</pre>

<p>This is a recurrent layer.</p>

<pre class="asciiart">
       y(0)              y(1)              y(2)    ...            y(t)
        ▲                 ▲                 ▲                      ▲            
        │                 │                 │                      │            
    ┌──────┐          ┌──────┐          ┌──────┐               ┌──────┐         
    │ Mem  │          │      │          │      │               │      │         
0──▶│ Cell │───H(0)──▶│      │───H(1)──▶│      │─ ... ─H(t-1)─▶│      │──H(t)──▶  
    └──────┘          └──────┘          └──────┘               └──────┘         
        ▲                 ▲                 ▲                      ▲            
        │                 │                 │                      │            
       x(0)              x(1)              x(2)                   x(t)   



──────────────────────────────────Time Steps───────────────────────────────▶    

</pre>

<p>The shapes of different variables are,</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">x(i)</code>:  (batch_size, dim), for univariate series, dim = 1</li>
  <li><code class="language-plaintext highlighter-rouge">y(i)</code>: (batch_size, units_number), the number of output neurons.</li>
  <li><code class="language-plaintext highlighter-rouge">H(i)</code>: this is the <em>state output</em>, for simple RNN, <code class="language-plaintext highlighter-rouge">H(i) = y(i)</code></li>
</ul>

<h2 id="the-model">The model</h2>

<p>The code for the model is</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">SimpleRNN</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># return the { y(i) } sequence
</span>        <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># None -&gt; input takes arbitrary length
</span>    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">SimpleRNN</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="c1"># return the { y(t) } value
</span>    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># forecasting one number
</span><span class="p">])</span>
</code></pre></div></div>

<p>The intermediate RNN layers should always have the parameter <code class="language-plaintext highlighter-rouge">return_sequence = True</code>.</p>

<p>We can also change the data in the mode, with a <code class="language-plaintext highlighter-rouge">Lamda</code> layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">SimpleRNN</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">SimpleRNN</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">100</span>  <span class="c1"># fit the scale of dataset
</span>    <span class="p">)</span>
<span class="p">])</span>

<span class="sh">"""</span><span class="s">model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
simple_rnn (SimpleRNN)       (None, None, 20)          440       
_________________________________________________________________
simple_rnn_1 (SimpleRNN)     (None, 20)                820       
_________________________________________________________________
dense (Dense)                (None, 1)                 21        
_________________________________________________________________
lambda (Lambda)              (None, 1)                 0         
=================================================================
Total params: 1,281
Trainable params: 1,281
Non-trainable params: 0
_________________________________________________________________
</span><span class="sh">"""</span>
</code></pre></div></div>

<h2 id="using-lstm">Using LSTM</h2>

<p>We can use an <code class="language-plaintext highlighter-rouge">LSTM</code> laryer rather than a <code class="language-plaintext highlighter-rouge">SimpleRNN</code> to construct the model.</p>

<p>There is another <em>cell state</em> in LSTM, which carries the information to more time steps.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Bidirectional</span><span class="p">(</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Bidirectional</span><span class="p">(</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="mf">100.0</span><span class="p">)</span>
<span class="p">])</span>

<span class="sh">"""</span><span class="s">model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
lambda (Lambda)              (None, None, 1)           0         
_________________________________________________________________
bidirectional (Bidirectional (None, None, 64)          8704      
_________________________________________________________________
bidirectional_1 (Bidirection (None, 64)                24832     
_________________________________________________________________
dense (Dense)                (None, 1)                 65        
_________________________________________________________________
lambda_1 (Lambda)            (None, 1)                 0         
=================================================================
Total params: 33,601
Trainable params: 33,601
Non-trainable params: 0
_________________________________________________________________
</span><span class="sh">"""</span>
</code></pre></div></div>

<h2 id="using-convolution">Using Convolution</h2>

<p>We can add another <code class="language-plaintext highlighter-rouge">Conv1D</code> layer, to improve the results.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv1D</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">causal</span><span class="sh">"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">200</span><span class="p">)</span>
<span class="p">])</span>

<span class="sh">"""</span><span class="s">model.summary()
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv1d (Conv1D)              (None, None, 32)          128       
_________________________________________________________________
lstm (LSTM)                  (None, None, 32)          8320      
_________________________________________________________________
lstm_1 (LSTM)                (None, None, 32)          8320      
_________________________________________________________________
dense (Dense)                (None, None, 1)           33        
_________________________________________________________________
lambda (Lambda)              (None, None, 1)           0         
=================================================================
Total params: 16,801
Trainable params: 16,801
Non-trainable params: 0
_________________________________________________________________
</span><span class="sh">"""</span>
</code></pre></div></div>


</div>


    </div>

  </body>

</html>
