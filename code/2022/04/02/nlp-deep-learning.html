<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Tensorflow Applications</title>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="stylesheet" href="/assets/css/styles/github-gist.css">
    <script src="/assets/js/toc.js"></script>
    <script src="/assets/js/highlight.pack.js"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true
        }
      });
    </script>

    <script type="text/javascript" id="MathJax-script" async
            src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"
      >
    </script>
    
    </head>

  <body>
    <script>hljs.initHighlightingOnLoad();</script>
    
    <nav>
    <ul>
        
        <li>
        <a href=/
            
        >
            Home
        </a>
        </li>
        
        <li>
        <a href=/notebook.html
            
        >
            Notebook
        </a>
        </li>
        
    </ul>
</nav>


    <div class="main">
        <div id="side_bar">
    02 Apr 2022
</div>

<div class="post center">
    <ul id="markdown-toc">
  <li><a href="#modelling-time-series" id="markdown-toc-modelling-time-series">Modelling Time Series</a>    <ul>
      <li><a href="#introduction" id="markdown-toc-introduction">Introduction</a></li>
      <li><a href="#typical-patterns" id="markdown-toc-typical-patterns">Typical Patterns</a>        <ul>
          <li><a href="#stationary" id="markdown-toc-stationary">Stationary</a></li>
          <li><a href="#non-stationary" id="markdown-toc-non-stationary">Non-stationary</a></li>
        </ul>
      </li>
      <li><a href="#prepare-data" id="markdown-toc-prepare-data">Prepare Data</a>        <ul>
          <li><a href="#tfdatadataset" id="markdown-toc-tfdatadataset">tf.data.Dataset</a></li>
          <li><a href="#training-data" id="markdown-toc-training-data">Training Data</a></li>
        </ul>
      </li>
      <li><a href="#train-models" id="markdown-toc-train-models">Train Models</a>        <ul>
          <li><a href="#baseline-models" id="markdown-toc-baseline-models">Baseline Models</a></li>
          <li><a href="#linear-regression" id="markdown-toc-linear-regression">Linear Regression</a></li>
          <li><a href="#mlp" id="markdown-toc-mlp">MLP</a></li>
          <li><a href="#simple-rnn" id="markdown-toc-simple-rnn">Simple RNN</a></li>
          <li><a href="#lstm" id="markdown-toc-lstm">LSTM</a></li>
        </ul>
      </li>
      <li><a href="#evaluate-prediction" id="markdown-toc-evaluate-prediction">Evaluate Prediction</a></li>
    </ul>
  </li>
  <li><a href="#modellling-natural-language" id="markdown-toc-modellling-natural-language">Modellling Natural Language</a>    <ul>
      <li><a href="#introduction-1" id="markdown-toc-introduction-1">Introduction</a></li>
      <li><a href="#tokenise" id="markdown-toc-tokenise">Tokenise</a></li>
      <li><a href="#learning-the-embedding" id="markdown-toc-learning-the-embedding">Learning the Embedding</a></li>
      <li><a href="#model-the-sequence" id="markdown-toc-model-the-sequence">Model the Sequence</a>        <ul>
          <li><a href="#lstm-1" id="markdown-toc-lstm-1">LSTM</a></li>
          <li><a href="#1d-cnn" id="markdown-toc-1d-cnn">1D CNN</a></li>
        </ul>
      </li>
      <li><a href="#generate-new-text" id="markdown-toc-generate-new-text">Generate New Text</a></li>
    </ul>
  </li>
</ul>

<hr />

<p>This is the note I took when taking the following two courses,</p>

<ul>
  <li><a href="https://www.coursera.org/learn/natural-language-processing-tensorflow/home/welcome">Tensorflow for NLP</a></li>
  <li><a href="https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/home/welcome">Tensorflow for time series</a></li>
</ul>

<hr />

<h2 id="modelling-time-series">Modelling Time Series</h2>

<h3 id="introduction">Introduction</h3>

<p>Times seires is defined as</p>

<blockquote>
  <p>an ordered sequence of values that are usually <em>equally spaced</em> over time</p>
</blockquote>

<p>We can use ML for the follwoing applications</p>

<ol>
  <li>Predicting the future (forecasting)</li>
  <li>Retracing the past (imputation &amp; Interpolate)</li>
  <li>Detection of the anomaly (spikes in time series)</li>
  <li>Detection of patterns</li>
</ol>

<h3 id="typical-patterns">Typical Patterns</h3>

<h4 id="stationary">Stationary</h4>

<p>Time series are ideally from a stationary stochastic process, with following patterns.</p>

<ol>
  <li>Trend (linear).</li>
  <li>Sensonality (periodicity).</li>
  <li>White noise (not learnable).</li>
  <li>Auto-correlation (often shown as <em>deterministic decay</em>)</li>
</ol>

<p>These four basic types are often combined for real data.</p>

<h4 id="non-stationary">Non-stationary</h4>

<p>In real life, the data may not be a <em>stationary random process</em>, so everything can change!</p>

<p>These dataset are called <em>none stationary time series</em>.</p>

<p>We chop a small and stable section of the entire time series, to make some predictions.</p>

<p>(more data may not be better!)</p>

<p>We just can not predict drastic changes!</p>

<h3 id="prepare-data">Prepare Data</h3>

<h4 id="tfdatadataset">tf.data.Dataset</h4>

<p>Tensorflow offered a nice <code class="language-plaintext highlighter-rouge">Dataset</code> class as the container for data.</p>

<p>Here are some examples.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">val</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="p">)</span>
<span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="sh">"""</span><span class="s"> result
0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>We can use the <code class="language-plaintext highlighter-rouge">window</code> method to generate nested (2D) dataset from 1D arrays.</p>

<p>Notice, the results can not be converted to <code class="language-plaintext highlighter-rouge">numpy</code> directly.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="c1"># data.numpy() -&gt; _VariantDataset' object has no attribute 'numpy'
</span>    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">val</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">()</span>

<span class="sh">"""</span><span class="s"> result
0, 1, 2, 3, 4, 
1, 2, 3, 4, 5, 
2, 3, 4, 5, 6, 
3, 4, 5, 6, 7, 
4, 5, 6, 7, 8, 
5, 6, 7, 8, 9, 
6, 7, 8, 9, 
7, 8, 9, 
8, 9, 
9, 
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>We can use <code class="language-plaintext highlighter-rouge">drop_remainder</code> to remove the result, whose length is smaller than widnow size.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">data</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="n">val</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">end</span><span class="o">=</span><span class="sh">"</span><span class="s">, </span><span class="sh">"</span><span class="p">)</span>
    <span class="nf">print</span><span class="p">()</span>

<span class="sh">"""</span><span class="s">result
0, 1, 2, 3, 4, 
1, 2, 3, 4, 5, 
2, 3, 4, 5, 6, 
3, 4, 5, 6, 7, 
4, 5, 6, 7, 8, 
5, 6, 7, 8, 9, 
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>We can also transofrm a block of array into <code class="language-plaintext highlighter-rouge">numpy</code> directly, using the <code class="language-plaintext highlighter-rouge">flat_map</code> method.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">data</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    
<span class="sh">"""</span><span class="s">result
[0 1 2 3 4]
[1 2 3 4 5]
[2 3 4 5 6]
[3 4 5 6 7]
[4 5 6 7 8]
[5 6 7 8 9]
</span><span class="sh">"""</span>
</code></pre></div></div>

<h4 id="training-data">Training Data</h4>

<p>For the sake of predicting time series, we split the data <code class="language-plaintext highlighter-rouge">1, 2, 3, 4, 5</code> into</p>

<ul>
  <li>The features <code class="language-plaintext highlighter-rouge">1, 2, 3, 4</code> (the prediction will base on these numbers)</li>
  <li>The label <code class="language-plaintext highlighter-rouge">5,</code> (the prediction will predict this value)</li>
</ul>

<p>This can be done with the <code class="language-plaintext highlighter-rouge">map</code> method</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]))</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    
<span class="sh">"""</span><span class="s">result
[0 1 2 3] [4]
[1 2 3 4] [5]
[2 3 4 5] [6]
[3 4 5 6] [7]
[4 5 6 7] [8]
[5 6 7 8] [9]
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>We can also <code class="language-plaintext highlighter-rouge">shuffle</code> the data, to avoid the <a href="https://www.coursera.org/learn/tensorflow-sequences-time-series-and-prediction/supplement/Qo0TU/sequence-bias"><em>sequence bias</em></a>, which is defined as</p>

<blockquote>
  <p>Sequence bias is when the order of things can impact the selection of things.</p>
</blockquote>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">numpy</span><span class="p">(),</span> <span class="n">y</span><span class="p">.</span><span class="nf">numpy</span><span class="p">())</span>
    
<span class="sh">"""</span><span class="s">result
[0 1 2 3] [4]
[5 6 7 8] [9]
[2 3 4 5] [6]
[1 2 3 4] [5]
[4 5 6 7] [8]
[3 4 5 6] [7]
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>Finally, we separate the dataset into different <em>batches</em>, for the training of the network.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">(</span><span class="n">x</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">x</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">:]))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">buffer_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="mi">2</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="k">for</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="nf">print</span><span class="p">(</span><span class="n">x</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="n">shape</span><span class="p">,</span> <span class="n">y</span><span class="p">.</span><span class="nf">numpy</span><span class="p">().</span><span class="n">shape</span><span class="p">)</span>
    
<span class="sh">"""</span><span class="s">result
(2, 4) (2, 1) -&gt; x = [[1, 2, 3, 4], [3, 4, 5, 6]], y = [[5,], [6,]]
(2, 4) (2, 1)
(2, 4) (2, 1)
</span><span class="sh">"""</span>
</code></pre></div></div>

<p>To generate dataset from an existing time series, we can use the following function, which includes all previous methods.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">windowed_dataset</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">window_size</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">shuffle_buffer</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Args:
        series (np.ndarray): 1D array
        window_size (int): the length + 1 of the features
        batch_size (int): the number of instances in a batch for training
        suffle_buffer (int): the buffer for randomnise dastaset
    </span><span class="sh">"""</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">data</span><span class="p">.</span><span class="n">Dataset</span><span class="p">.</span><span class="nf">from_tensor_slices</span><span class="p">(</span><span class="n">series</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">window</span><span class="p">(</span><span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">shift</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">flat_map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">window</span><span class="p">:</span> <span class="n">window</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="n">window_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">shuffle</span><span class="p">(</span><span class="n">shuffle_buffer</span><span class="p">).</span><span class="nf">map</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">window</span><span class="p">:</span> <span class="p">(</span><span class="n">window</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">window</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
    <span class="p">)</span>
    <span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="p">.</span><span class="nf">batch</span><span class="p">(</span><span class="n">batch_size</span><span class="p">).</span><span class="nf">prefetch</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dataset</span>
</code></pre></div></div>

<h3 id="train-models">Train Models</h3>

<h4 id="baseline-models">Baseline Models</h4>

<p>There are ways to predict time series with out ML, called the <em>statistical forcasting</em>.</p>

<ul>
  <li>Naive forcasting: $\hat{y}_{i+1} = y_i$ (the result in that course is just wrong.)</li>
  <li>Moving average: $\hat{y}<em>i = \frac1S\sum</em>{j=0}^{j=S} \hat{y}_{i-j} $</li>
  <li>Differencing: try to predict the difference of the time series, rather than the time series.
(Differencing removes the <em>trend</em> and the <em>seasonality</em>)</li>
</ul>

<h4 id="linear-regression">Linear Regression</h4>

<p>We can use linear regression to fit the data, by using a NN with one layer. Mathematically, the calculation is written as</p>

\[y_i = \left( \begin{matrix}
w_1 \\ w_2 \\ \vdots \\ w_n
\end{matrix} \right)
\left( \begin{matrix} 
y_{i+(0-n)} &amp; y_{i+(1-n)} &amp; \dots &amp; y_{i+(n-1-n)}
\end{matrix} \right) + b.\]

<p>The code for such model is,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">window_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">shuffle_buffer_size</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">l0</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="n">window_size</span><span class="p">])</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span><span class="n">l0</span><span class="p">])</span>
</code></pre></div></div>

<h4 id="mlp">MLP</h4>

<p>We can use a deeper network to fit the data, with minimum modification. This yields slightly better result.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">window_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">shuffle_buffer_size</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="n">window_size</span><span class="p">]),</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">),</span>
  <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<h4 id="simple-rnn">Simple RNN</h4>

<p>Here is the structure of a simple RNN.</p>

<pre class="asciiart">
                     output (forecasts)
               shape: (batch_size, time_steps, )
                            ▲         
                            │         
                  ┌──────────────────┐
                  │   Dense Layer    │
                  └──────────────────┘
                            ▲         
                            │         
                  ┌──────────────────┐
                  │ Recurrent Layer  │
                  └──────────────────┘
                            ▲         
                            │         
                  ┌──────────────────┐
                  │ Recurrent Layer  │
                  └──────────────────┘
                            ▲         
                            │         
                         input x
      shape (batch_size, time_steps, dimensions)
</pre>

<p>This is a recurrent layer.</p>

<pre class="asciiart">
       y(0)              y(1)              y(2)    ...            y(t)
        ▲                 ▲                 ▲                      ▲            
        │                 │                 │                      │            
    ┌──────┐          ┌──────┐          ┌──────┐               ┌──────┐         
    │ Mem  │          │      │          │      │               │      │         
0──▶│ Cell │───H(0)──▶│      │───H(1)──▶│      │─ ... ─H(t-1)─▶│      │──H(t)──▶  
    └──────┘          └──────┘          └──────┘               └──────┘         
        ▲                 ▲                 ▲                      ▲            
        │                 │                 │                      │            
       x(0)              x(1)              x(2)                   x(t)   



──────────────────────────────────Time Steps───────────────────────────────▶    

</pre>

<p>The shapes of different variables are,</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">x(i)</code>:  (batch_size, dim), for univariate series, dim = 1</li>
  <li><code class="language-plaintext highlighter-rouge">y(i)</code>: (batch_size, units_number), the number of output neurons.</li>
  <li><code class="language-plaintext highlighter-rouge">H(i)</code>: this is the <em>state output</em>, for simple RNN, <code class="language-plaintext highlighter-rouge">H(i) = y(i)</code></li>
</ul>

<p>The code for a simple RNN model is</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">SimpleRNN</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
        <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>  <span class="c1"># return the { y(i) } sequence
</span>        <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>  <span class="c1"># None -&gt; input takes arbitrary length
</span>    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">SimpleRNN</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>  <span class="c1"># y(t)
</span>    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># forecasting one number
</span><span class="p">])</span>
</code></pre></div></div>

<p>The intermediate RNN layers should always have the parameter <code class="language-plaintext highlighter-rouge">return_sequence = True</code>.</p>

<p>We can also change the data in the mode, with a <code class="language-plaintext highlighter-rouge">Lamda</code> layer.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">SimpleRNN</span><span class="p">(</span>
        <span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">SimpleRNN</span><span class="p">(</span><span class="n">units</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">False</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">100</span>  <span class="c1"># fit the scale of dataset
</span>    <span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<h4 id="lstm">LSTM</h4>

<p>We can use an <code class="language-plaintext highlighter-rouge">LSTM</code> laryer rather than a <code class="language-plaintext highlighter-rouge">SimpleRNN</code> to construct the model.</p>

<p>There is another <em>cell state</em> in LSTM, which carries the information to more time steps.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span>
        <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">tf</span><span class="p">.</span><span class="nf">expand_dims</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Bidirectional</span><span class="p">(</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Bidirectional</span><span class="p">(</span>
        <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">)</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="mf">100.0</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<p>We can also add <code class="language-plaintext highlighter-rouge">Conv1D</code> layer on top of the <code class="language-plaintext highlighter-rouge">LSTM</code>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">models</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv1D</span><span class="p">(</span>
        <span class="n">filters</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">strides</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">padding</span><span class="o">=</span><span class="sh">"</span><span class="s">causal</span><span class="sh">"</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">"</span><span class="s">relu</span><span class="sh">"</span><span class="p">,</span>
        <span class="n">input_shape</span><span class="o">=</span><span class="p">[</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">,</span> <span class="n">return_sequences</span><span class="o">=</span><span class="bp">True</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Lambda</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span> <span class="o">*</span> <span class="mi">200</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<h3 id="evaluate-prediction">Evaluate Prediction</h3>

<p>There are following matrices</p>

\[\begin{aligned}
\mathrm{mse} &amp;= \frac1N \sum_i (\hat{y}_i - y_i)^2 \\
\mathrm{rmse} &amp;= \sqrt{\mathrm{mse}} \\
\mathrm{mae} &amp;= \frac1N \sum_i \vert \hat{y}_i - y_i \vert \\
\mathrm{mape} &amp;= \frac1N \sum_i \vert \frac{\hat{y}_i - y_i}{y_i} \vert  &amp; \text{p = percentage}
\end{aligned}\]

<ul>
  <li>$\hat{y}_i$ is the predicted value at time $i$</li>
  <li>$y_i$ is the true value at time point $i$</li>
</ul>

<h2 id="modellling-natural-language">Modellling Natural Language</h2>

<h3 id="introduction-1">Introduction</h3>

<p>Natural language processing (NLP) allows us to extract information from languages.</p>

<p>For example, let’s say we want to predict if a movie comment is positive.</p>

<p>Our dataset would contains,</p>

<ul>
  <li>$n$ Comments, and</li>
  <li>Each comment will contain $m$ words, and</li>
  <li>Each comment will be either <strong>positive</strong> or <strong>negative</strong>.</li>
</ul>

<h3 id="tokenise">Tokenise</h3>

<p>To analyse the natural languages, we need to convert text to numbers.
We treat every comment in the same way.
And for each comment, we will</p>

<ol>
  <li>Give each word a number/code. This is called <strong>encoding</strong>.</li>
  <li>We decide what to do if we see a new word, known as outer vocabulary (oov).</li>
  <li>We will <strong>pad</strong> the comment so that all of them have equal length, ie word counts.</li>
</ol>

<p>This is relevant code in tensorflow using the class <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer">tokenisor</a> and function <a href="https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences">pad_sequences</a>.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequence</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># only give the top 10,000 most frequent words a number/code
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">120</span>    <span class="c1"># maximum length (word count) for each comment
</span><span class="n">trunc_type</span><span class="o">=</span><span class="sh">'</span><span class="s">post</span><span class="sh">'</span>   <span class="c1"># removing excess words at the end of the comments
</span><span class="n">oov_tok</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;OOV&gt;</span><span class="sh">"</span>   <span class="c1"># use &lt;OOV&gt; to represnt words never seen before
</span>
<span class="n">comments</span> <span class="o">=</span> <span class="p">...</span>  <span class="c1"># a list of n comments (strings), each comment has different length
</span>
<span class="c1"># generate the encoding rule that convert words to numbers
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="n">oov_tok</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>

<span class="c1"># convert words to numbers
</span><span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>

<span class="c1"># making the sequences so that they have equal length
</span><span class="n">padded</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="n">trunc_type</span><span class="p">)</span>

<span class="c1"># obtain the map between words and numbers, the dict is {words: number}
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>
</code></pre></div></div>

<h3 id="learning-the-embedding">Learning the Embedding</h3>

<p>The tokenised sequences does not make much scence.
Word <em>cat</em> may have the code of <code class="language-plaintext highlighter-rouge">00001</code> and word <em>cats</em> might have <code class="language-plaintext highlighter-rouge">95843</code> but these two words should somehow be similar.</p>

<p>To get meaningful representations of words, we need to represnt the words as vectores.
And the words with similar meanings should be <strong>closer</strong> in their corresponding space, where the vectors be in.
The conversion from <strong>code</strong> to <strong>vector</strong> is carried out by <strong>embedding</strong>.</p>

<p>We get the embedding by <em>training a predictive network</em>. Here is the relevent code.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># only give the top 10,000 most frequent words a number/code
</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># each word is represented by 16 numbers
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">120</span>    <span class="c1"># maximum length (word count) for each comment
</span><span class="n">trunc_type</span><span class="o">=</span><span class="sh">'</span><span class="s">post</span><span class="sh">'</span>   <span class="c1"># removing excess words at the end of the comments
</span><span class="n">oov_tok</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;OOV&gt;</span><span class="sh">"</span>   <span class="c1"># use &lt;OOV&gt; to represnt words never seen before
</span>
<span class="n">comments</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">comment</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">hello world</span><span class="sh">"</span><span class="p">]</span>  <span class="c1"># n comments (strings) with different lengths
</span>
<span class="c1"># generate the encoding rule that convert words to numbers
</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="n">oov_tok</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>

<span class="c1"># convert words to numbers
</span><span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>

<span class="c1"># making the sequences so that they have equal length
</span><span class="n">padded</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="n">trunc_type</span><span class="p">)</span>

<span class="c1"># obtain the map between words and numbers, the dict is {words: number}
</span><span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>

<span class="c1"># construct a model with embedding
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Flatten</span><span class="p">(),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">binary_crossentropy</span><span class="sh">'</span><span class="p">,</span><span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span><span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">])</span>
<span class="n">model</span><span class="p">.</span><span class="nf">summary</span><span class="p">()</span>

</code></pre></div></div>

<p>This is the summary of the model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Model</span><span class="p">:</span> <span class="sh">"</span><span class="s">sequential</span><span class="sh">"</span>
<span class="n">_________________________________________________________________</span>
<span class="nc">Layer </span><span class="p">(</span><span class="nb">type</span><span class="p">)</span>                 <span class="n">Output</span> <span class="n">Shape</span>              <span class="n">Param</span> <span class="c1">#   
</span><span class="o">=================================================================</span>
<span class="nf">embedding </span><span class="p">(</span><span class="n">Embedding</span><span class="p">)</span>        <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">120</span><span class="p">,</span> <span class="mi">16</span><span class="p">)</span>           <span class="mi">160000</span>     
<span class="c1"># meaning: 120 vectors, each vector is 16 dimensional
</span><span class="n">_________________________________________________________________</span>
<span class="nf">flatten </span><span class="p">(</span><span class="n">Flatten</span><span class="p">)</span>            <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1920</span><span class="p">)</span>              <span class="mi">0</span>     
<span class="n">_________________________________________________________________</span>
<span class="nf">dense </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>                <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>                 <span class="mi">11526</span> 
<span class="n">_________________________________________________________________</span>
<span class="nf">dense_1 </span><span class="p">(</span><span class="n">Dense</span><span class="p">)</span>              <span class="p">(</span><span class="bp">None</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>                 <span class="mi">7</span>
<span class="o">=================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">171</span><span class="p">,</span><span class="mi">533</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">171</span><span class="p">,</span><span class="mi">533</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">_________________________________________________________________</span>
</code></pre></div></div>

<p>The final “product” of the embeding layer is the <strong>weight</strong>.
In this exampe, the shape of the weight can be obtained via,</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># only give the top 10,000 most frequent words a number/code
</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># each word is represented by 16 numbers
</span><span class="p">....</span><span class="bp">...</span>
<span class="n">embed_layer</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">weights</span> <span class="o">=</span> <span class="n">embed_layer</span><span class="p">.</span><span class="nf">weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>  <span class="c1"># shape (10000, 16), 
</span></code></pre></div></div>

<p>The weights will map every word from their <strong>code</strong> to a <strong>vector</strong>.</p>

<h3 id="model-the-sequence">Model the Sequence</h3>

<p>We need consider the <strong>sequence</strong> of the language into consideration.</p>

<h4 id="lstm-1">LSTM</h4>

<p>A very popular model called long short term memory (LSTM) can be used to construct an RNN.
The structure of the model is different but the idea is similar.</p>

<pre class="asciiart">
    .───────────.                .───────────.                 .───────────.           
   (    y(0)     )              (    y(1)     )               (    y(2)     )          
    `───────────'                `───────────'                 `───────────'           
          ▲                            ▲                             ▲                 
  ━━━━━━━━╋━━━━━━━Cell State━━━━━━━━━━━╋━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╋━━━━━━━━━━━━▶    
          │                            │                             │                 
┌───────────────────┐        ┌───────────────────┐         ┌───────────────────┐       
│     Function      │───────▶│     Function      │────────▶│     Function      │──────▶
└───────────────────┘        └───────────────────┘         └───────────────────┘       
          ▲                            ▲                             ▲                 
          │                            │                             │                 
          │                            │                             │                 
    .───────────.                .───────────.                 .───────────.           
   (    x(0)     )              (    x(1)     )               (    x(2)     )          
    `───────────'                `───────────'                 `───────────'           
</pre>

<p>LSTM can be think of as an “update” to RNN.
There is an additional pipeline called “cell state” being passed from earilier states.
The cell state can also be bi-direcitonal, where the future can affect the past.</p>

<p>The following code use the LSTM in the model in tensorflow.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">tensorflow</span> <span class="k">as</span> <span class="n">tf</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="n">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>

<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>  <span class="c1"># only give the top 10,000 most frequent words a number/code
</span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">16</span>  <span class="c1"># each word is represented by 16 numbers
</span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">120</span>    <span class="c1"># maximum length (word count) for each comment
</span><span class="n">trunc_type</span><span class="o">=</span><span class="sh">'</span><span class="s">post</span><span class="sh">'</span>   <span class="c1"># removing excess words at the end of the comments
</span><span class="n">oov_tok</span> <span class="o">=</span> <span class="sh">"</span><span class="s">&lt;OOV&gt;</span><span class="sh">"</span>   <span class="c1"># use &lt;OOV&gt; to represnt words never seen before
</span>
<span class="n">comments</span> <span class="o">=</span> <span class="p">[</span><span class="sh">"</span><span class="s">comment</span><span class="sh">"</span><span class="p">,</span> <span class="sh">"</span><span class="s">hello world</span><span class="sh">"</span><span class="p">]</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">oov_token</span><span class="o">=</span><span class="n">oov_tok</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>
<span class="n">sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">(</span><span class="n">comments</span><span class="p">)</span>
<span class="n">padded</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncating</span><span class="o">=</span><span class="n">trunc_type</span><span class="p">)</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span>

<span class="c1"># construct a model with sequence model
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">64</span><span class="p">)),</span>  <span class="c1"># use LSTM
</span>    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<h4 id="1d-cnn">1D CNN</h4>

<p>Except for the LSTM, we can also use a 1D convolution layer, like below</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># construct a model convolution rather than LSTM
</span><span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="nc">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">),</span>
  	<span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Conv1D</span><span class="p">(</span><span class="mi">128</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">GlobalMaxPooling1D</span><span class="p">(),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">relu</span><span class="sh">'</span><span class="p">),</span>
    <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">layers</span><span class="p">.</span><span class="nc">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">sigmoid</span><span class="sh">'</span><span class="p">)</span>
<span class="p">])</span>
</code></pre></div></div>

<h3 id="generate-new-text">Generate New Text</h3>

<p>We can train a network to generate new text. The idea is to <em>predict the next word</em> based on existing texts.</p>

<p>The following code present an example for this task.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tokenizer</span> <span class="o">=</span> <span class="nc">Tokenizer</span><span class="p">()</span>

<span class="n">data</span><span class="o">=</span><span class="sh">"""</span><span class="s">
In the town of Athy one Jeremy Lanigan
Battered away til he hadnt a pound.
His father died and made him a man again
Left him a farm and ten acres of ground.
He gave a grand party for friends and relations
Who didnt forget him when come to the wall,
And if youll but listen Ill make your eyes glisten
Of the rows and the ructions of Lanigans Ball.
Myself to be sure got free invitation,
For all the nice girls and boys I might ask,
</span><span class="gp">...</span>
<span class="sh">"""</span>

<span class="n">corpus</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="nf">lower</span><span class="p">().</span><span class="nf">split</span><span class="p">(</span><span class="sh">"</span><span class="se">\n</span><span class="sh">"</span><span class="p">)</span>

<span class="n">tokenizer</span><span class="p">.</span><span class="nf">fit_on_texts</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>
<span class="n">total_words</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span>

<span class="c1"># print(tokenizer.word_index) -&gt; 263
# print(total_words) -&gt; {'and': 1, 'the': 2, 'a': 3, ... }
</span>
<span class="n">input_sequences</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">corpus</span><span class="p">:</span>
	<span class="n">token_list</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">([</span><span class="n">line</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">token_list</span><span class="p">)):</span>
		<span class="n">n_gram_sequence</span> <span class="o">=</span> <span class="n">token_list</span><span class="p">[:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span>
		<span class="n">input_sequences</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">n_gram_sequence</span><span class="p">)</span>

<span class="c1"># pad sequences 
</span><span class="n">max_sequence_len</span> <span class="o">=</span> <span class="nf">max</span><span class="p">([</span><span class="nf">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">input_sequences</span><span class="p">])</span>
<span class="n">input_sequences</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">array</span><span class="p">(</span>
  <span class="nf">pad_sequences</span><span class="p">(</span><span class="n">input_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># create predictors and label
</span><span class="n">xs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">input_sequences</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_sequences</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>

<span class="c1"># from numbers to categorical label
</span><span class="n">ys</span> <span class="o">=</span> <span class="n">tf</span><span class="p">.</span><span class="n">keras</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="nf">to_categorical</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="n">total_words</span><span class="p">)</span>

<span class="sh">"""</span><span class="s">
xs[6] -&gt; [ 0  0  0  4  2 66  8 67 68 69] (history)
labels[6] -&gt;  70  (prediction)

categorical label
ys[6].shape -&gt; (263,)
ys[6][70] -&gt; 1
ys[6][65] -&gt; 0
</span><span class="sh">"""</span>

<span class="c1"># Train the network to make prediction
</span><span class="n">model</span> <span class="o">=</span> <span class="nc">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Embedding</span><span class="p">(</span><span class="n">total_words</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">input_length</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Bidirectional</span><span class="p">(</span><span class="nc">LSTM</span><span class="p">(</span><span class="mi">20</span><span class="p">)))</span>
<span class="n">model</span><span class="p">.</span><span class="nf">add</span><span class="p">(</span><span class="nc">Dense</span><span class="p">(</span><span class="n">total_words</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="sh">'</span><span class="s">softmax</span><span class="sh">'</span><span class="p">))</span>

<span class="n">model</span><span class="p">.</span><span class="nf">compile</span><span class="p">(</span>
  <span class="n">loss</span><span class="o">=</span><span class="sh">'</span><span class="s">categorical_crossentropy</span><span class="sh">'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="sh">'</span><span class="s">adam</span><span class="sh">'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="sh">'</span><span class="s">accuracy</span><span class="sh">'</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">fit</span><span class="p">(</span><span class="n">xs</span><span class="p">,</span> <span class="n">ys</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


<span class="c1"># Use the network to generate text, starting with she
</span><span class="n">seed_text</span> <span class="o">=</span> <span class="sh">"</span><span class="s">She</span><span class="sh">"</span>
<span class="n">next_words</span> <span class="o">=</span> <span class="mi">10</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">next_words</span><span class="p">):</span>
	<span class="n">token_list</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">.</span><span class="nf">texts_to_sequences</span><span class="p">([</span><span class="n">seed_text</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
	<span class="n">token_list</span> <span class="o">=</span> <span class="nf">pad_sequences</span><span class="p">([</span><span class="n">token_list</span><span class="p">],</span> <span class="n">maxlen</span><span class="o">=</span><span class="n">max_sequence_len</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="sh">'</span><span class="s">pre</span><span class="sh">'</span><span class="p">)</span>
	<span class="n">predicted</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict</span><span class="p">(</span><span class="n">token_list</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
	<span class="n">predicted</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">argmax</span><span class="p">(</span><span class="n">predicted</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
	<span class="n">output_word</span> <span class="o">=</span> <span class="sh">""</span>
	<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">index</span> <span class="ow">in</span> <span class="n">tokenizer</span><span class="p">.</span><span class="n">word_index</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
		<span class="k">if</span> <span class="n">index</span> <span class="o">==</span> <span class="n">predicted</span><span class="p">:</span>
			<span class="n">output_word</span> <span class="o">=</span> <span class="n">word</span>
			<span class="k">break</span>
	<span class="n">seed_text</span> <span class="o">+=</span> <span class="sh">"</span><span class="s"> </span><span class="sh">"</span> <span class="o">+</span> <span class="n">output_word</span>
<span class="nf">print</span><span class="p">(</span><span class="n">seed_text</span><span class="p">)</span>

<span class="c1"># -&gt; She stepped out and i stepped in again again again again
</span></code></pre></div></div>


</div>


    </div>

  </body>

</html>
